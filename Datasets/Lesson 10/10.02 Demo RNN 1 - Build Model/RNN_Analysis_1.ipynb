{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64473ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 23:05:08.555698: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-24 23:05:08.555768: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import imdb  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Embedding  \n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import os  \n",
    "from sklearn.metrics import roc_auc_score, roc_curve  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf1ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory\n",
    "output_dir = 'model_output/dense'\n",
    "\n",
    "# training:\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "# embedding: \n",
    "n_dim = 64\n",
    "n_unique_words = 5000  \n",
    "n_words_to_skip = 50  \n",
    "max_review_length = 100\n",
    "pad_type = trunc_type = 'pre'\n",
    "\n",
    "# dense \n",
    "n_dense = 64\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b83e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = imdb.load_data(num_words=n_unique_words, \n",
    "                                                        skip_top=n_words_to_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1305070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([2, 2, 2, 2, 2, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 2, 173, 2, 256, 2, 2, 100, 2, 838, 112, 50, 670, 2, 2, 2, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 2, 2, 172, 4536, 1111, 2, 546, 2, 2, 447, 2, 192, 50, 2, 2, 147, 2025, 2, 2, 2, 2, 1920, 4613, 469, 2, 2, 71, 87, 2, 2, 2, 530, 2, 76, 2, 2, 1247, 2, 2, 2, 515, 2, 2, 2, 626, 2, 2, 2, 62, 386, 2, 2, 316, 2, 106, 2, 2, 2223, 2, 2, 480, 66, 3785, 2, 2, 130, 2, 2, 2, 619, 2, 2, 124, 51, 2, 135, 2, 2, 1415, 2, 2, 2, 2, 215, 2, 77, 52, 2, 2, 407, 2, 82, 2, 2, 2, 107, 117, 2, 2, 256, 2, 2, 2, 3766, 2, 723, 2, 71, 2, 530, 476, 2, 400, 317, 2, 2, 2, 2, 1029, 2, 104, 88, 2, 381, 2, 297, 98, 2, 2071, 56, 2, 141, 2, 194, 2, 2, 2, 226, 2, 2, 134, 476, 2, 480, 2, 144, 2, 2, 2, 51, 2, 2, 224, 92, 2, 104, 2, 226, 65, 2, 2, 1334, 88, 2, 2, 283, 2, 2, 4472, 113, 103, 2, 2, 2, 2, 2, 178, 2]),\n",
       "       list([2, 194, 1153, 194, 2, 78, 228, 2, 2, 1463, 4369, 2, 134, 2, 2, 715, 2, 118, 1634, 2, 394, 2, 2, 119, 954, 189, 102, 2, 207, 110, 3103, 2, 2, 69, 188, 2, 2, 2, 2, 2, 249, 126, 93, 2, 114, 2, 2300, 1523, 2, 647, 2, 116, 2, 2, 2, 2, 229, 2, 340, 1322, 2, 118, 2, 2, 130, 4901, 2, 2, 1002, 2, 89, 2, 952, 2, 2, 2, 455, 2, 2, 2, 2, 1543, 1905, 398, 2, 1649, 2, 2, 2, 163, 2, 3215, 2, 2, 1153, 2, 194, 775, 2, 2, 2, 349, 2637, 148, 605, 2, 2, 2, 123, 125, 68, 2, 2, 2, 349, 165, 4362, 98, 2, 2, 228, 2, 2, 2, 1157, 2, 299, 120, 2, 120, 174, 2, 220, 175, 136, 50, 2, 4373, 228, 2, 2, 2, 656, 245, 2350, 2, 2, 2, 131, 152, 491, 2, 2, 2, 2, 1212, 2, 2, 2, 371, 78, 2, 625, 64, 1382, 2, 2, 168, 145, 2, 2, 1690, 2, 2, 2, 1355, 2, 2, 2, 52, 154, 462, 2, 89, 78, 285, 2, 145, 95]),\n",
       "       list([2, 2, 2, 2, 2, 2, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 2, 71, 149, 2, 2, 112, 2, 2401, 311, 2, 2, 3711, 2, 75, 2, 1829, 296, 2, 86, 320, 2, 534, 2, 263, 4821, 1301, 2, 1873, 2, 89, 78, 2, 66, 2, 2, 360, 2, 2, 58, 316, 334, 2, 2, 1716, 2, 645, 662, 2, 257, 85, 1200, 2, 1228, 2578, 83, 68, 3912, 2, 2, 165, 1539, 278, 2, 69, 2, 780, 2, 106, 2, 2, 1338, 2, 2, 2, 2, 215, 2, 610, 2, 2, 87, 326, 2, 2300, 2, 2, 2, 2, 272, 2, 57, 2, 2, 2, 2, 2, 2, 2307, 51, 2, 170, 2, 595, 116, 595, 1352, 2, 191, 79, 638, 89, 2, 2, 2, 2, 106, 607, 624, 2, 534, 2, 227, 2, 129, 113]),\n",
       "       list([2, 2, 2, 2, 2, 2804, 2, 2040, 432, 111, 153, 103, 2, 1494, 2, 70, 131, 67, 2, 61, 2, 744, 2, 3715, 761, 61, 2, 452, 2, 2, 985, 2, 2, 59, 166, 2, 105, 216, 1239, 2, 1797, 2, 2, 2, 2, 744, 2413, 2, 2, 2, 687, 2, 2, 2, 2, 2, 3693, 2, 2, 2, 121, 59, 456, 2, 2, 2, 265, 2, 575, 111, 153, 159, 59, 2, 1447, 2, 2, 586, 482, 2, 2, 96, 59, 716, 2, 2, 172, 65, 2, 579, 2, 2, 2, 1615, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 464, 2, 314, 2, 2, 2, 719, 605, 2, 2, 202, 2, 310, 2, 3772, 3501, 2, 2722, 58, 2, 2, 537, 2116, 180, 2, 2, 413, 173, 2, 263, 112, 2, 152, 377, 2, 537, 263, 846, 579, 178, 54, 75, 71, 476, 2, 413, 263, 2504, 182, 2, 2, 75, 2306, 922, 2, 279, 131, 2895, 2, 2867, 2, 2, 2, 921, 2, 192, 2, 1219, 3890, 2, 2, 217, 4122, 1710, 537, 2, 1236, 2, 736, 2, 2, 61, 403, 2, 2, 2, 61, 4494, 2, 2, 4494, 159, 90, 263, 2311, 4319, 309, 2, 178, 2, 82, 4319, 2, 65, 2, 2, 145, 143, 2, 2, 2, 537, 746, 537, 537, 2, 2, 2, 2, 594, 2, 2, 94, 2, 3987, 2, 2, 2, 2, 538, 2, 1795, 246, 2, 2, 2, 2, 635, 2, 2, 51, 408, 2, 94, 318, 1382, 2, 2, 2, 2683, 936, 2, 2, 2, 2, 2, 2, 2, 1885, 2, 1118, 2, 80, 126, 842, 2, 2, 2, 2, 4726, 2, 4494, 2, 1550, 3633, 159, 2, 341, 2, 2733, 2, 4185, 173, 2, 90, 2, 2, 2, 2, 2, 1784, 86, 1117, 2, 3261, 2, 2, 2, 2, 2, 2, 2841, 2, 2, 1010, 2, 793, 2, 2, 1386, 1830, 2, 2, 246, 50, 2, 2, 2750, 1944, 746, 90, 2, 2, 2, 124, 2, 882, 2, 882, 496, 2, 2, 2213, 537, 121, 127, 1219, 130, 2, 2, 494, 2, 124, 2, 882, 496, 2, 341, 2, 2, 846, 2, 2, 2, 2, 1906, 2, 97, 2, 236, 2, 1311, 2, 2, 2, 2, 2, 2, 2, 91, 2, 3987, 70, 2, 882, 2, 579, 2, 2, 2, 2, 2, 537, 2, 2, 2, 2, 65, 2, 537, 75, 2, 1775, 3353, 2, 1846, 2, 2, 2, 154, 2, 2, 518, 53, 2, 2, 2, 3211, 882, 2, 399, 2, 75, 257, 3807, 2, 2, 2, 2, 456, 2, 65, 2, 2, 205, 113, 2, 2, 2, 2, 2, 2, 2, 242, 2, 91, 1202, 2, 2, 2070, 307, 2, 2, 2, 126, 93, 2, 2, 2, 188, 1076, 3222, 2, 2, 2, 2, 2348, 537, 2, 53, 537, 2, 82, 2, 2, 2, 2, 2, 280, 2, 219, 2, 2, 431, 758, 859, 2, 953, 1052, 2, 2, 2, 2, 94, 2, 2, 238, 60, 2, 2, 2, 804, 2, 2, 2, 2, 132, 2, 67, 2, 2, 2, 2, 283, 2, 2, 2, 2, 2, 242, 955, 2, 2, 279, 2, 2, 2, 1685, 195, 2, 238, 60, 796, 2, 2, 671, 2, 2804, 2, 2, 559, 154, 888, 2, 726, 50, 2, 2, 2, 2, 566, 2, 579, 2, 64, 2574]),\n",
       "       list([2, 249, 1323, 2, 61, 113, 2, 2, 2, 1637, 2, 2, 56, 2, 2401, 2, 457, 88, 2, 2626, 1400, 2, 3171, 2, 70, 79, 2, 706, 919, 2, 2, 355, 340, 355, 1696, 96, 143, 2, 2, 2, 289, 2, 61, 369, 71, 2359, 2, 2, 2, 131, 2073, 249, 114, 249, 229, 249, 2, 2, 2, 126, 110, 2, 473, 2, 569, 61, 419, 56, 429, 2, 1513, 2, 2, 534, 95, 474, 570, 2, 2, 124, 138, 88, 2, 421, 1543, 52, 725, 2, 61, 419, 2, 2, 1571, 2, 1543, 2, 2, 2, 2, 2, 296, 2, 3524, 2, 2, 421, 128, 74, 233, 334, 207, 126, 224, 2, 562, 298, 2167, 1272, 2, 2601, 2, 516, 988, 2, 2, 79, 120, 2, 595, 2, 784, 2, 3171, 2, 165, 170, 143, 2, 2, 2, 2, 2, 226, 251, 2, 61, 113]),\n",
       "       list([2, 778, 128, 74, 2, 630, 163, 2, 2, 1766, 2, 1051, 2, 2, 85, 156, 2, 2, 148, 139, 121, 664, 665, 2, 2, 1361, 173, 2, 749, 2, 2, 3804, 2, 2, 226, 65, 2, 2, 127, 2, 2, 2, 2])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc5ee800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[0:6]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51c0fec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44d398d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05465124",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"PAD\"] = 0\n",
    "word_index[\"START\"] = 1\n",
    "word_index[\"UNK\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fb53d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {v:k for k,v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f9c467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 2,\n",
       " 173,\n",
       " 2,\n",
       " 256,\n",
       " 2,\n",
       " 2,\n",
       " 100,\n",
       " 2,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 480,\n",
       " 284,\n",
       " 2,\n",
       " 150,\n",
       " 2,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 2,\n",
       " 2,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 2,\n",
       " 546,\n",
       " 2,\n",
       " 2,\n",
       " 447,\n",
       " 2,\n",
       " 192,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 147,\n",
       " 2025,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 2,\n",
       " 2,\n",
       " 71,\n",
       " 87,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 530,\n",
       " 2,\n",
       " 76,\n",
       " 2,\n",
       " 2,\n",
       " 1247,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 515,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 626,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 62,\n",
       " 386,\n",
       " 2,\n",
       " 2,\n",
       " 316,\n",
       " 2,\n",
       " 106,\n",
       " 2,\n",
       " 2,\n",
       " 2223,\n",
       " 2,\n",
       " 2,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 2,\n",
       " 2,\n",
       " 130,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 619,\n",
       " 2,\n",
       " 2,\n",
       " 124,\n",
       " 51,\n",
       " 2,\n",
       " 135,\n",
       " 2,\n",
       " 2,\n",
       " 1415,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 215,\n",
       " 2,\n",
       " 77,\n",
       " 52,\n",
       " 2,\n",
       " 2,\n",
       " 407,\n",
       " 2,\n",
       " 82,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 107,\n",
       " 117,\n",
       " 2,\n",
       " 2,\n",
       " 256,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3766,\n",
       " 2,\n",
       " 723,\n",
       " 2,\n",
       " 71,\n",
       " 2,\n",
       " 530,\n",
       " 476,\n",
       " 2,\n",
       " 400,\n",
       " 317,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1029,\n",
       " 2,\n",
       " 104,\n",
       " 88,\n",
       " 2,\n",
       " 381,\n",
       " 2,\n",
       " 297,\n",
       " 98,\n",
       " 2,\n",
       " 2071,\n",
       " 56,\n",
       " 2,\n",
       " 141,\n",
       " 2,\n",
       " 194,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 226,\n",
       " 2,\n",
       " 2,\n",
       " 134,\n",
       " 476,\n",
       " 2,\n",
       " 480,\n",
       " 2,\n",
       " 144,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 51,\n",
       " 2,\n",
       " 2,\n",
       " 224,\n",
       " 92,\n",
       " 2,\n",
       " 104,\n",
       " 2,\n",
       " 226,\n",
       " 65,\n",
       " 2,\n",
       " 2,\n",
       " 1334,\n",
       " 88,\n",
       " 2,\n",
       " 2,\n",
       " 283,\n",
       " 2,\n",
       " 2,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 178,\n",
       " 2]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8fe9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"UNK UNK UNK UNK UNK brilliant casting location scenery story direction everyone's really suited UNK part UNK played UNK UNK could UNK imagine being there robert UNK UNK UNK amazing actor UNK now UNK same being director UNK father came UNK UNK same scottish island UNK myself UNK UNK loved UNK fact there UNK UNK real connection UNK UNK UNK UNK witty remarks throughout UNK UNK were great UNK UNK UNK brilliant UNK much UNK UNK bought UNK UNK UNK soon UNK UNK UNK released UNK UNK UNK would recommend UNK UNK everyone UNK watch UNK UNK fly UNK UNK amazing really cried UNK UNK end UNK UNK UNK sad UNK UNK know what UNK say UNK UNK cry UNK UNK UNK UNK must UNK been good UNK UNK definitely UNK also UNK UNK UNK two little UNK UNK played UNK UNK UNK norman UNK paul UNK were UNK brilliant children UNK often left UNK UNK UNK UNK list UNK think because UNK stars UNK play them UNK grown up UNK such UNK big UNK UNK UNK whole UNK UNK these children UNK amazing UNK should UNK UNK UNK what UNK UNK done don't UNK think UNK whole story UNK UNK lovely because UNK UNK true UNK UNK someone's life after UNK UNK UNK UNK UNK us UNK\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_word[id] for id in x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8adc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_x_train,_),(all_x_valid,_) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0adb032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_word[id] for id in all_x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa659f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_review_length, \n",
    "                        padding=pad_type, truncating=trunc_type, value=0)\n",
    "x_valid = pad_sequences(x_valid, maxlen=max_review_length, \n",
    "                        padding=pad_type, truncating=trunc_type, value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c16ece41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1415,    2,    2,    2,    2,  215,    2,   77,   52,    2,    2,\n",
       "         407,    2,   82,    2,    2,    2,  107,  117,    2,    2,  256,\n",
       "           2,    2,    2, 3766,    2,  723,    2,   71,    2,  530,  476,\n",
       "           2,  400,  317,    2,    2,    2,    2, 1029,    2,  104,   88,\n",
       "           2,  381,    2,  297,   98,    2, 2071,   56,    2,  141,    2,\n",
       "         194,    2,    2,    2,  226,    2,    2,  134,  476,    2,  480,\n",
       "           2,  144,    2,    2,    2,   51,    2,    2,  224,   92,    2,\n",
       "         104,    2,  226,   65,    2,    2, 1334,   88,    2,    2,  283,\n",
       "           2,    2, 4472,  113,  103,    2,    2,    2,    2,    2,  178,\n",
       "           2],\n",
       "       [ 163,    2, 3215,    2,    2, 1153,    2,  194,  775,    2,    2,\n",
       "           2,  349, 2637,  148,  605,    2,    2,    2,  123,  125,   68,\n",
       "           2,    2,    2,  349,  165, 4362,   98,    2,    2,  228,    2,\n",
       "           2,    2, 1157,    2,  299,  120,    2,  120,  174,    2,  220,\n",
       "         175,  136,   50,    2, 4373,  228,    2,    2,    2,  656,  245,\n",
       "        2350,    2,    2,    2,  131,  152,  491,    2,    2,    2,    2,\n",
       "        1212,    2,    2,    2,  371,   78,    2,  625,   64, 1382,    2,\n",
       "           2,  168,  145,    2,    2, 1690,    2,    2,    2, 1355,    2,\n",
       "           2,    2,   52,  154,  462,    2,   89,   78,  285,    2,  145,\n",
       "          95],\n",
       "       [1301,    2, 1873,    2,   89,   78,    2,   66,    2,    2,  360,\n",
       "           2,    2,   58,  316,  334,    2,    2, 1716,    2,  645,  662,\n",
       "           2,  257,   85, 1200,    2, 1228, 2578,   83,   68, 3912,    2,\n",
       "           2,  165, 1539,  278,    2,   69,    2,  780,    2,  106,    2,\n",
       "           2, 1338,    2,    2,    2,    2,  215,    2,  610,    2,    2,\n",
       "          87,  326,    2, 2300,    2,    2,    2,    2,  272,    2,   57,\n",
       "           2,    2,    2,    2,    2,    2, 2307,   51,    2,  170,    2,\n",
       "         595,  116,  595, 1352,    2,  191,   79,  638,   89,    2,    2,\n",
       "           2,    2,  106,  607,  624,    2,  534,    2,  227,    2,  129,\n",
       "         113],\n",
       "       [   2,    2,    2,  188, 1076, 3222,    2,    2,    2,    2, 2348,\n",
       "         537,    2,   53,  537,    2,   82,    2,    2,    2,    2,    2,\n",
       "         280,    2,  219,    2,    2,  431,  758,  859,    2,  953, 1052,\n",
       "           2,    2,    2,    2,   94,    2,    2,  238,   60,    2,    2,\n",
       "           2,  804,    2,    2,    2,    2,  132,    2,   67,    2,    2,\n",
       "           2,    2,  283,    2,    2,    2,    2,    2,  242,  955,    2,\n",
       "           2,  279,    2,    2,    2, 1685,  195,    2,  238,   60,  796,\n",
       "           2,    2,  671,    2, 2804,    2,    2,  559,  154,  888,    2,\n",
       "         726,   50,    2,    2,    2,    2,  566,    2,  579,    2,   64,\n",
       "        2574],\n",
       "       [   2,    2,  131, 2073,  249,  114,  249,  229,  249,    2,    2,\n",
       "           2,  126,  110,    2,  473,    2,  569,   61,  419,   56,  429,\n",
       "           2, 1513,    2,    2,  534,   95,  474,  570,    2,    2,  124,\n",
       "         138,   88,    2,  421, 1543,   52,  725,    2,   61,  419,    2,\n",
       "           2, 1571,    2, 1543,    2,    2,    2,    2,    2,  296,    2,\n",
       "        3524,    2,    2,  421,  128,   74,  233,  334,  207,  126,  224,\n",
       "           2,  562,  298, 2167, 1272,    2, 2601,    2,  516,  988,    2,\n",
       "           2,   79,  120,    2,  595,    2,  784,    2, 3171,    2,  165,\n",
       "         170,  143,    2,    2,    2,    2,    2,  226,  251,    2,   61,\n",
       "         113],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    2,  778,  128,   74,    2,  630,  163,    2,    2,\n",
       "        1766,    2, 1051,    2,    2,   85,  156,    2,    2,  148,  139,\n",
       "         121,  664,  665,    2,    2, 1361,  173,    2,  749,    2,    2,\n",
       "        3804,    2,    2,  226,   65,    2,    2,  127,    2,    2,    2,\n",
       "           2]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d58ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for x in x_train[0:6]:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23f8519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD UNK begins better than UNK ends funny UNK UNK russian UNK crew UNK UNK other actors UNK UNK those scenes where documentary shots UNK UNK spoiler part UNK message UNK UNK contrary UNK UNK whole story UNK UNK does UNK UNK UNK UNK'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_word[id] for id in x_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42b3400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 64)           320000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                409664    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 729,729\n",
      "Trainable params: 729,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 23:21:02.846177: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-24 23:21:02.846685: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-24 23:21:02.846720: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-24 23:21:02.846750: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hbot): /proc/driver/nvidia/version does not exist\n",
      "2021-08-24 23:21:02.847337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-24 23:21:02.847701: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words, n_dim, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3052e025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 5000, 320000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim, n_unique_words, n_dim*n_unique_words # embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7733fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64, 6400)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_review_length, n_dim, n_dim*max_review_length # flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0f10a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 409664)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dense, n_dim*max_review_length*n_dense + n_dense # dense: weights + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe767f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dense + 1 # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7ddd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db587e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-24 23:25:49.701891: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-08-24 23:25:49.706896: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2494080000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "196/196 [==============================] - 6s 27ms/step - loss: 0.6540 - accuracy: 0.5808 - val_loss: 0.3520 - val_accuracy: 0.8426\n",
      "Epoch 2/4\n",
      "196/196 [==============================] - 4s 21ms/step - loss: 0.2919 - accuracy: 0.8859 - val_loss: 0.3638 - val_accuracy: 0.8420\n",
      "Epoch 3/4\n",
      "196/196 [==============================] - 11s 56ms/step - loss: 0.1391 - accuracy: 0.9569 - val_loss: 0.4259 - val_accuracy: 0.8342\n",
      "Epoch 4/4\n",
      "196/196 [==============================] - 5s 24ms/step - loss: 0.0352 - accuracy: 0.9936 - val_loss: 0.5301 - val_accuracy: 0.8347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6682aa40d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, epochs=epochs, verbose=1, \n",
    "          validation_data=(x_valid, y_valid), \n",
    "          callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5de416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "model.load_weights(output_dir+\"/weights.02.hdf5\") # NOT zero-indexed\n",
    "y_hat = model.predict_proba(x_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276dd372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa2997b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f4e43e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'92.92'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_auc = roc_auc_score(y_valid, y_hat)*100.0\n",
    "\"{:0.2f}\".format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "381be92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db40bf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"START please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_word[id] for id in all_x_valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9febd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
