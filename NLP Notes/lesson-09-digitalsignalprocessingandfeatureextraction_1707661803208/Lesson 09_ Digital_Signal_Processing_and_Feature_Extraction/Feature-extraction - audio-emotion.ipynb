{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> **Feature Extraction - Audio-emotions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "- Understand techniques for extracting useful features from audio.\n",
    "- Explore the Mel-frequency cepstral coefficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id= 'f0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Introduction](#f1)<br>\n",
    "[2. MFCC](#f2)<br>\n",
    "[3. Deepdive](#f3)<br>\n",
    "[4. Statistical features](#f4)<br>\n",
    "[5. Take aways](#f5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'f1'>\n",
    "<font size = 10 color = 'midnightblue'> **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "\n",
    "Broadly speaking, there are two categories of features:\n",
    "\n",
    "**- Time domain features**<br>\n",
    "> These are simpler to extract and understand, such as the energy of the signal, zero-crossing rate, maximum amplitude, minimum energy, etc.\n",
    "\n",
    "**- Frequency-based features**<br>\n",
    "> These are obtained by converting the time-based signal into the frequency domain. \n",
    "> While they are harder to comprehend, extra information, such as pitch, rhythms, melody, etc., can be provided. \n",
    "> Check the infographic below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src = \"https://www.nti-audio.com/portals/0/pic/news/FFT-Time-Frequency-View-540.png\">\n",
    "\n",
    "<font size = 2>\n",
    "    \n",
    "<b>The time vs frequency domain image sourced from __[here](https://www.nti-audio.com/en/support/know-how/fast-fourier-transform-fft)__</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<font size = 4> \n",
    "    \n",
    "**Note about the data used:**\n",
    "- The data used here is the `Ryerson Audio-Visual Database of Emotional Speech and Song or RAVDESS`.\n",
    "- The section of data used here comprises **1440 files**, resulting from **60 trials per actor multiplied by 24 actors**.\n",
    "- The RAVDESS features 24 professional actors, evenly divided between 12 females and 12 males.\n",
    "- These actors vocalize two lexically-matched statements in a neutral North American accent.\n",
    "- Speech emotions included in the dataset encompass calm, happy, sad, angry, fearful, surprise, and disgust expressions.\n",
    "- Each expression is generated at two levels of emotional intensity, namely normal and strong, with an additional neutral expression.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'f2'>\n",
    "<font size = 10 color = 'midnightblue'>**MFCC: Mel-frequency cepstral coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "\n",
    "- MFCC, or Mel-frequency cepstral coefficient, is a significant representation of the sound-producing vocal tract.\n",
    "- In the Mel-scale, pitch distinctions are perceived equally, aligning with human auditory preferences.\n",
    "- Derived from a sound's short-term power spectrum in Mel-scale, MFCCs are commonly used in speech recognition for capturing distinct voice frequencies.\n",
    "- In machine learning, a prevalent approach is to treat MFCC as an 'image' and utilize it as a feature, enabling transfer learning for enhanced information.\n",
    "- While effective, research suggests that statistics from MFCCs or other domains can also contain substantial information. Both methods will be explored.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<font size = 4>\n",
    "\n",
    "<b>Check</b> [this article](https://medium.com/prathena/the-dummys-guide-to-mfcc-aceab2450fd) <b>to learn more about MFCC.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color = seagreen> **Load neccessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import os\n",
    "import IPython.display as ipd  # To play sound in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'f3'>\n",
    "<font size = 10 color = 'midnightblue'>**Deepdive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "- Choose specific instances for MFCC visualization.\n",
    "- Select two distinct emotions and genders for assessment, playing the chosen instances to evaluate audio data quality.\n",
    "- This initial evaluation provides insights into the potential success of the classifier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Female; Emotion - Angry \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "# audio wave\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.waveshow(X, sr=sample_rate)\n",
    "plt.title('Audio sampled at 44100 hrz')\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Male; Emotion - Angry \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "# audio wave\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.waveshow(X, sr=sample_rate)\n",
    "plt.title('Audio sampled at 44100 hrz')\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "<center> <b>The male counterpart responded very calmly....<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Female; Emotion - Happy \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "# audio wave\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.waveshow(X, sr=sample_rate)\n",
    "plt.title('Audio sampled at 44100 hrz')\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Male; Emotion - Happy \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "mfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "\n",
    "# audio wave\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.waveshow(X, sr=sample_rate)\n",
    "plt.title('Audio sampled at 44100 hrz')\n",
    "\n",
    "# MFCC\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.specshow(mfcc, x_axis='time')\n",
    "plt.ylabel('MFCC')\n",
    "plt.colorbar()\n",
    "\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'f4'>\n",
    "<font size = 10 color = 'midnightblue'>**Statistical features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "\n",
    "- Having examined the MFCC output in the format of a 2D matrix with bands on the y-axis and time on the x-axis, we will simplify by calculating mean values across each band over time (i.e., row means).\n",
    "- The most distinctive feature is noticeable in the first band at the bottom of the MFCC plot, where observed changes over time are minimal due to the short time window.\n",
    "- The crucial aspect is capturing information across bands, illustrated by plotting the mean of each band as a time series.\n",
    "- A comparison will be conducted between Angry female and Angry male for the same uttered sentence.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Female; Emotion - Angry \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "print(len(female))\n",
    "\n",
    "# Source - RAVDESS; Gender - Male; Emotion - Angry \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "print(len(male))\n",
    "\n",
    "# audio wave\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(female, label='female')\n",
    "plt.plot(male, label='male')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "When the same sentence is spoken, a noticeable distinction between male and female voices emerges, with females generally having a higher pitch. \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color = seagreen><center> <b>Let's explore further:<br>\n",
    "\n",
    "<font size = 6 color = seagreen> <center> <b>Compare a <b><font color = 'gray'> Happy Female</b> and a <font color = 'gray'><b>Happy Male</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source - RAVDESS; Gender - Female; Emotion - happy \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "female = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "female = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "print(len(female))\n",
    "\n",
    "# Source - RAVDESS; Gender - Male; Emotion - happy \n",
    "path = \"/voc/work/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\n",
    "X, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \n",
    "male = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n",
    "male = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\n",
    "print(len(male))\n",
    "\n",
    "# Plot the two audio waves together\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(female, label='female')\n",
    "plt.plot(male, label='male')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'f4'>\n",
    "<font size = 10 color = 'midnightblue'>**Take Aways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "- As demonstrated earlier, the utilization of MFCC is effective in distinguishing gender and emotions.\n",
    "- Despite the omission of substantial information by focusing only on the mean, it seems we capture enough to discern differences.\n",
    "- The exploration of whether these differences are significant for distinguishing various emotions will be undertaken in the next section, where we will establish a baseline emotion classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#f0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
