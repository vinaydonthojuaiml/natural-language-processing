{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWJnDTYnDnZB",
    "tags": []
   },
   "source": [
    "# <center> <font size=24 color= 'steelblue'> **Sequence Models**\n",
    "# <center><img src = \"https://drive.google.com/uc?export=view&id=1_KIQA0fg8V1f4VygalHlBBQ2orYJZUq2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id= 'r0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Recurrent neural network ](#r1)<br>\n",
    "> [1.1 RNN refresher](#r1.1)<br>\n",
    "> [1.2 RNN limitations](#r1.2)<br>\n",
    "\n",
    "[2. LSTM ](#r2)<br>\n",
    "[3. Bi-directional recurrent neural network](#r3)<br>\n",
    "[4. Building the models](#r4)<br>\n",
    "[5. Takeaways](#r5)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCnHk68uDnZB"
   },
   "source": [
    "###### <a id = 'r1'>\n",
    "<font size = 10 color = 'midnightblue'>**Recurrent Neural Networks (RNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 'r1.1'>\n",
    "<font size = 6 color = pwdrblue> <b>RNN: Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCnHk68uDnZB"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "    \n",
    "**RNN can handle a stream of data sequentially. This is the way we humans are used to dealing with sequential inputs like text.**\n",
    "\n",
    "- The hidden layer of this network exhibits self-connection, earning it the term \"recurrent.\" Essentially, the activation of a subsequent word depends not only on its own input but also on the activation of the preceding word. <br>\n",
    "- This mechanism enables the network to encapsulate the influence of prior words on subsequent ones, enhancing the comprehension of meaning, akin to how human understanding unfolds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCnHk68uDnZB"
   },
   "source": [
    "<center> <img src = https://www.researchgate.net/profile/Subburam-Rajaram/publication/324680970/figure/fig1/AS:617962817986561@1524345225985/A-recurrent-neural-network-and-the-unfolding-in-time-of-the-computation-involved-in-its.png/Time_unfoldment.png>\n",
    "\n",
    "<center> <font size = 4 > <b>The illustration below depicting the temporal unfolding of an RNN vividly clarifies this concept.</b>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCnHk68uDnZB"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size=4 >\n",
    "    \n",
    "- The output may or may not be available at every time step, but the other units remain unaffected.\n",
    "- In each recurrent layer, neurons establish complete connections with themselves.\n",
    "- For instance, with three neurons in a hidden layer, there would be a total of 9 connections within the layer.\n",
    "- A temporal shift occurs, meaning that in the initial pass, we have a straightforward hidden layer.\n",
    "- However, in subsequent passes, the output activations from the previous pass influence the outputs, creating a cascading effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### <a id = 'r1.2'>\n",
    "<font size = 6 color = pwdrblue> **Limitations - RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8J8lKHoNbOl"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 >\n",
    "\n",
    "- BPTT (Backpropagation Through Time) emphasizes later sequence layers.\n",
    "- With increasing depth, the impact of the previous neuron on the output diminishes due to continual thresholding and the compounding effect thereof. \n",
    "    - This issue extends to forward propagation, affecting error back-propagation as well.\n",
    "    - Consequently, weight updates are minimally influenced by words earlier in the sequence compared to those appearing later.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "###### <a id = 'r2'>\n",
    "<font size = 10 color = 'midnightblue'>**LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 >\n",
    "    \n",
    "- LSTM, short for Long Short Term Memory, tackles the previously discussed problem.\n",
    "- It suggests incorporating connections from past layers to future ones in a systematic manner.\n",
    "- The neural network learns the strength of these connections, retaining the robust ones and discarding others.\n",
    "- This mechanism closely mimics our sequential processing of textual information.\n",
    "- When reading a passage, we progress through it word by word, akin to this approach.\n",
    "- Likewise, when faced with a particular task, our memory adjusts to meet the specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 >\n",
    "\n",
    "**Consider a scenario**\n",
    "- A passage is provided detailing the impacts of global warming over the past four decades.\n",
    "- Am inquiry is now generated as: <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "<center><font size = 4 color = seagreen> <b><i>`What were the consequences of global warming on marine ecosystems in 1984? Identify the pivotal events in the unfolding tragedy.`<b>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 >   \n",
    "\n",
    "**To answer this query,** <br>\n",
    "\n",
    "- It will be required to systematically read the passage, focusing on the mention of 1984 and relevant events from that timeframe.\n",
    "- Progressing through the sentences, information related to 1984 is naturally retained and the details unrelated to the specified year are forgotten.\n",
    "- In LSTM, such an architecture is implemented using gates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "# <center> <img src=\"https://drive.google.com/uc?export=view&id=19cqa917fL3b1XfN9nUy1e3INa9VFKEDO\" style=\"border: 3px solid  gray;\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4pLorm6NbOm"
   },
   "source": [
    "||||\n",
    "|-|-|-|\n",
    "|<font size = 5 color = 'midnightblue'> **Input Gate (Write Gate)** |<font size = 4.7 color = \\color>Decides whether the incoming input should be considered or not.|\n",
    "<font size = 5 color = 'midnightblue'> **Forget Gate (Memory Gate)** |<font size = 4.7 color = \\color> Decides whether to retain the memory in subsequent step.|\n",
    "<font size = 5 color = 'midnightblue'> **Output Gate (Read Gate)** |<font size = 4.7 color = \\color> Controls the flow of information from one cell to another.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6drrBgQNbOm"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "    \n",
    "- The Neural Network learns the values of these gates, and being sigmoid activated, they fall within the real-valued range of 0 to 1.\n",
    "- This implies that their states are not strictly binary but can vary.\n",
    "- An alternative method to achieve the same functionality as LSTM is the Gated Recurrent Unit (GRU). \n",
    "- It is a simpler model compared to LSTM, and as a result, many applications that once utilized LSTM are now transitioning to GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaIYnYRyDnZP",
    "tags": []
   },
   "source": [
    "###### <a id = 'r3'>\n",
    "<font size = 10 color = 'midnightblue'>**Bi-directional recurrent neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaIYnYRyDnZP",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "\n",
    "- While humans typically read sentences sequentially, they possess the ability to mentally backtrack to earlier portions as new information surfaces. <br>\n",
    "- Humans can adeptly handle information presented in a less-than-optimal order. Enabling your model to similarly backtrack across the input is facilitated by bidirectional recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxpDP5QbDnZP"
   },
   "source": [
    "# <center> <img src = \"https://upload.wikimedia.org/wikipedia/commons/3/35/Structural_diagrams_of_unidirectional_and_bidirectional_recurrent_neural_networks.png\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPUsFFEdDnZP"
   },
   "source": [
    "# <center> <img src=\"https://drive.google.com/uc?export=view&id=1FkmDT01c86RzYbfKsLKvOWhY4Tq7LOx-\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 'r4'>\n",
    "<font size = 10 color = 'midnightblue'>**Build the models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6drrBgQNbOm"
   },
   "source": [
    "<font size = 5 color = 'pwdrblue'> \n",
    "<b>Build an RNN model to perform movie review analysis using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MgublV7DnZC",
    "tags": []
   },
   "source": [
    "<font size = 5 color = seagreen> <b> Download the original dataset from the Stanford AI department [ðŸ”—](https://ai.stanford.edu/%7eamaas/data/sentiment/).<br>\n",
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "    \n",
    "1. <font size = 4> This is a dataset compiled for the 2011 paper Learning Word Vectors for Sentiment Analysis.<br>\n",
    "2. <font size = 4> It has a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t09b9eEADnZD"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5PMEwWwDnZD",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import glob\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJZ4N0kJDnZE"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Prepare the data for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOxH5FhqDnZF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process_data(filepath):\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "\n",
    "    shuffle(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34kpJijnDnZF"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Checking working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5p6FEmFBDnZF",
    "outputId": "7cea05c2-2814-47ff-dde1-17d59ae645f9"
   },
   "outputs": [],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj7603wkDnZG"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Loading pre-trained word vectors from the `Google News dataset` using the Word2Vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4AGYrSLDnZG"
   },
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('/voc/work/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNDmWZlEDnZG"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Combine the tokenizer and vectorizer into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-8xKUOaDnZH"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDIyYIkmNbOs"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Get the word vectors for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryC8dGxuDnZH",
    "outputId": "172076b0-e250-4278-b4e4-4b8b246b1e16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_vectors[\"dog\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9I0pjzmDnZH"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Unzip the target variable into separate (but corresponding) samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zo3OxgzDnZH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF0p8M-wDnZH"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Decompress the downloaded data\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "    \n",
    "- Uncomment the below code to decompress.<br>\n",
    "- Replace the path with respective paths to the file on your machine/lab.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35lZqnjXDnZH"
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# # open file\n",
    "# file = tarfile.open('/voc/work/aclImdb_v1.tar.gz')\n",
    "# #\n",
    "# # extracting file\n",
    "# file.extractall('/voc/work')\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcYN3Qp3DnZH"
   },
   "source": [
    "<font size = 5 color = seagreen> <b> Data preprocessing - using the `pre_process_data()`, created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSH7ID4mDnZH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = pre_process_data('/voc/work/aclImdb/train')\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWDBFoKwDnZH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPHjLG8sDnZI",
    "outputId": "e572d5d6-7787-4498-fcc6-303e58d1ab20"
   },
   "outputs": [],
   "source": [
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saWxPEBzDnZI"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "    \n",
    "**Note:**\n",
    "This step is an optional step and need not be used in case of availability required resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhVymCUWDnZI"
   },
   "outputs": [],
   "source": [
    "x_train_s = x_train[:1000]\n",
    "x_test_s = x_test[:200]\n",
    "y_train_s = y_train[:1000]\n",
    "y_test_s = y_test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SldC0BMDnZI"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Declare the hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypcvIxHJDnZI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxlen = 400            # arbitrary sequence length\n",
    "batch_size = 32         # Number of sample sequences to pass through (and aggregate the error) before backpropagating\n",
    "embedding_dims = 300    # from pre-trained word2vec model\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHc_r3TqDnZI"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Prepare the data by making each point of uniform length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ka6Eb-e0DnZI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXSCIqHNDnZI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_s = pad_trunc(x_train_s, maxlen)\n",
    "x_test_s = pad_trunc(x_test_s, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDywm01uDnZI"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Reshape into a numpy data structure for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffYtMuGRDnZI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_s = np.reshape(x_train_s, (len(x_train_s), maxlen, embedding_dims))\n",
    "y_train_s = np.array(y_train_s)\n",
    "x_test_s = np.reshape(x_test_s, (len(x_test_s), maxlen, embedding_dims))\n",
    "y_test_s = np.array(y_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjCfOJ_dDnZI"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Build a model: Start with a standard `Sequential()`, that is the layered Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmdrmBZZDnZI",
    "outputId": "81435e0c-301d-4697-a052-d7919bfd3ffb"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "\n",
    "model = Sequential()  # Initialize an empty Keras network\n",
    "\n",
    "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims))) # add recurrent layer\n",
    "model.add(Dropout(.2)) # adding a drop-out layer\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy']) # compile recurrent network\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmLN66NrDnZJ",
    "outputId": "a2e90514-4c32-4933-ae4e-fc5c747dc4c1"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_s, y_train_s,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_s, y_test_s))\n",
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"simplernn_weights1.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXxJLp2-DnZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"simplernn_model1.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('simplernn_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vyyzvK1DnZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_1 = \"I hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knHm6mE2DnZJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyBh_b5IDnZJ",
    "outputId": "10d38e83-0d85-498d-f03e-c1ac50fdc078"
   },
   "outputs": [],
   "source": [
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8vKlurWDnZO",
    "outputId": "76a64e07-84cc-4d34-f3a0-4bbfc19fabde"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n",
    "\n",
    "num_neurons = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2D4Yh7KDnZO",
    "outputId": "02a099fe-7991-472d-c50c-3cda86fcb227"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_s, y_train_s,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_s, y_test_s))\n",
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"simplernn_weights2.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaIYnYRyDnZP",
    "tags": []
   },
   "source": [
    "<font size = 6 color = 'pwdrblue'>**Build the bi-directional recurrent neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaIYnYRyDnZP",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">   \n",
    "<font size=4 > \n",
    "\n",
    "- While humans typically read sentences sequentially, they possess the ability to mentally backtrack to earlier portions as new information surfaces. <br>\n",
    "- Humans can adeptly handle information presented in a less-than-optimal order. Enabling your model to similarly backtrack across the input is facilitated by bidirectional recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxpDP5QbDnZP"
   },
   "source": [
    "# <center> <img src = \"https://upload.wikimedia.org/wikipedia/commons/3/35/Structural_diagrams_of_unidirectional_and_bidirectional_recurrent_neural_networks.png\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPUsFFEdDnZP"
   },
   "source": [
    "# <center> <img src=\"https://drive.google.com/uc?export=view&id=1FkmDT01c86RzYbfKsLKvOWhY4Tq7LOx-\" width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYG0EJBnDnZP",
    "outputId": "2cba383d-4d2b-47e9-ed90-353e00b1dfcf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "num_neurons = 10\n",
    "maxlen = 100\n",
    "embedding_dims = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(\n",
    "      num_neurons, return_sequences=True),\\\n",
    "    input_shape=(maxlen, embedding_dims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOiIDBlfDnZP",
    "tags": []
   },
   "source": [
    "<font size = 6 color = 'pwdrblue'> **Enhancing memory retention using long short-term memory networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoKj_N4UNbO9"
   },
   "source": [
    "# <center> <img src = \"https://upload.wikimedia.org/wikipedia/commons/9/93/LSTM_Cell.svg\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omheVgPnDnZP",
    "outputId": "1cba4071-a837-4651-d6d0-0b8503e121ab"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJk17zXFDnZP",
    "outputId": "24a755c2-8345-4cde-cfe1-e50647daa92b"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_s, y_train_s,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_s, y_test_s))\n",
    "\n",
    "model_structure = model.to_json()\n",
    "\n",
    "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights1.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZVH07KBDnZP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"lstm_model1.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('lstm_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEu7ilNgDnZP",
    "outputId": "3f096e64-dc65-41db-ee15-20998159a4c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_1 = \"I hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n",
    "\n",
    "# Pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "\n",
    "print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict(test_vec)))\n",
    "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAVPNOLDnZP",
    "tags": []
   },
   "source": [
    "##### <a id = 'r5'>\n",
    "<font size = 10 color = 'midnightblue'>**Take aways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAVPNOLDnZP"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "- **Temporal Importance in Natural Language Sequences:**\n",
    "\n",
    "> Recognizing the relevance of preceding elements, whether words or characters, is vital for the model's understanding of natural language sequences.\n",
    "\n",
    "- **Temporal Dimension Splitting:**\n",
    "\n",
    "> Dividing a natural language statement along the temporal dimension of tokens allows the machine to deepen its comprehension of language.\n",
    "\n",
    "- **Challenges in RNN Gradients:**\n",
    "\n",
    "> RNNs, being inherently deep, face challenges with gradients, such as vanishing or exploding gradients, making them particularly sensitive.\n",
    "\n",
    "- **Efficient Modeling with RNNs:**\n",
    "    \n",
    "> Efficiently modeling natural language character sequences became feasible with the application of recurrent neural networks to the task.\n",
    "    \n",
    "- **Aggregate Weight Adjustment:**\n",
    "    \n",
    "> In an RNN, weights are adjusted collectively across time for a given sample, contributing to the network's understanding of temporal dependencies.\n",
    "    \n",
    "- **Methods for Analyzing RNN Outputs:**\n",
    "\n",
    "> Various methods, including simultaneous backward and forward processing through an RNN, can be employed to examine the output of recurrent neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
