{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font size = 24 color = 'steelblue'>**Speech Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src= 'https://drive.google.com/uc?export=view&id=1ACG-2N9lRygp6no9MJTJJYbeAuuOiUtW' height = 500, width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "\n",
    "* Learn to preprocess audio files\n",
    "* Understand audio features\n",
    "* Identify methods of speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id= 's0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of contents:**<br>\n",
    "[1. Data description](#s1)<br>\n",
    "[2. Preprocessing of the audio files](#s2)<br>\n",
    "[3. General audio features exploration](#s3)<br>\n",
    "[4. Speech analysis](#s4)<br>\n",
    "[5. Sentiment analysis](#s5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 's1'>\n",
    "<font size = 10 color = 'midnightblue'> **Data Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "**Audio files used in this analysis are available in Wikimedia:**\n",
    "\n",
    "- VOA News Report: \"FIFA to Re-Examine Video Replays After World Cup Referee Mistakes\" https://commons.wikimedia.org/wiki/File:2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.ogg\n",
    "- Aldi speaking Bible verse in Bahasa Indonesia https://commons.wikimedia.org/wiki/File:Aldi_-_Indonesian_language_-_Bible_Verse_John_3-16.ogg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen > **Load the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import generic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 's2'>\n",
    "<font size = 10 color = 'midnightblue'> **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "- For processing in most Python audio-processing libraries, audio files typically need to be in waveform (.wav) format.\n",
    "- WAV is preferred due to its lossless attributes, unlike compressed formats such as mp3/m4a.\n",
    "- Hence, it is recommended to convert your audio file to WAV before analysis if it's not already in that format.\n",
    "- In Python, audio file conversion is simplified by the pydub package.\n",
    "- To achieve this, create an AudioSegment instance from the input file and utilize the \"export\" function to generate the desired output file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color = pwdrblue> **Creating a clone to enable `ogg` to `wav` conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://git.ffmpeg.org/ffmpeg.git ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the OGG file\n",
    "ogg_file = \"/voc/work/2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.ogg\"\n",
    "audio = AudioSegment.from_file(ogg_file, format=\"ogg\")\n",
    "\n",
    "# Export as WAV\n",
    "wav_file = \"2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav\"\n",
    "audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "print(f\"Conversion complete. WAV file saved at {wav_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"/voc/work/2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 's3'>\n",
    "<font size = 10 color = 'midnightblue'> **General audio features explorations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen> **Amplitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('/voc/work//2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav', mono=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "img = librosa.display.waveshow(y, sr=sr, ax=ax)\n",
    "\n",
    "ax.set(title='Envelope view, stereo')\n",
    "ax.label_outer()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen> **Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file to Librosa\n",
    "y, sr = librosa.load('/voc/work/2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav')\n",
    "\n",
    "# Converts data into short term Fourier transform. \n",
    "# STFT converts signals such that we can know the amplitude of the given frequency at a given time\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "\n",
    "# Display spectogram\n",
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "img = librosa.display.specshow(D, y_axis='linear', x_axis='time', sr=sr)\n",
    "ax.set(title='Linear-frequency power spectrogram')\n",
    "ax.label_outer()\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 3))\n",
    "img = librosa.display.specshow(D, y_axis='log', x_axis='time', sr=sr)\n",
    "ax.set(title='Logarithmic-frequency power spectrogram')\n",
    "ax.label_outer()\n",
    "\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 's4'>\n",
    "<font size = 10 color = 'midnightblue'> **Speech Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --user SpeechRecognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4 color = seagreen><b>Speech recognition</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4> \n",
    "    \n",
    "- Input the audio file(s) into a predefined engine for textual transcription.\n",
    "- Utilize the speech recognition library in python for this transcription task.\n",
    "- The library supports various engines, including CMU Sphinx, Google Cloud Speech API, Microsoft Bing Voice Recognition, and IBM Speech to Text.\n",
    "- It's important to note that certain engines may require API tokens.\n",
    "- For this project, **Google Speech Recognition** is used with the default API key.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4 color = seagreen> **Key Features of Speech Recognition:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4> \n",
    "    \n",
    "> The Speech Recognition library encompasses a Recognizer class, equipped with a set of built-in functions for configuring speech recognition settings and accessing features.\n",
    "    \n",
    "- To begin, we import the library and configure the Recognizer. <br>\n",
    "- Subsequently, we import the audio file intended for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Set up recognizer\n",
    "r = sr.Recognizer()\n",
    "r.energy_threshold = 300\n",
    "\n",
    "# Import Audio data\n",
    "test_audio = sr.AudioFile('/voc/work/2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav')\n",
    "with test_audio as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    audio = r.record(source)\n",
    "    \n",
    "r.recognize_google(audio, show_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4> \n",
    "\n",
    "- Transcription can now be directly performed using the recognize_google function, which takes the audio file as input and produces the transcription as output. \n",
    "- However, depending on the audio file quality, several other functionalities can be employed to improve the audio file, thus resulting in enhanced transcription.\n",
    "- If unnecessary content is present in your audio file that does not require transcription, the duration and offset variables in the record function can be utilized to specify the particular segment of the audio file to transcribe.\n",
    "- In the case of a noisy audio file, the adjust_for_ambient_noise function can be employed to adjust the energy threshold for ambient noise levels. This calibration enables the noise to be disregarded, allowing the recognizer to concentrate on the actual speech.\n",
    "- The code for speech recognition on the VOA News audio file is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Audio data\n",
    "\n",
    "test_audio2 = sr.AudioFile('/voc/work/2010-06-29_VOA_News_report_-_FIFA_to_Re-Examine_Video_Replays_After_World_Cup_Referee_Mistakes.wav')\n",
    "with test_audio2 as source2:\n",
    "    audio2 = r.record(source2, duration=60)\n",
    "    audio3 = r.record(source2, duration=60)\n",
    "    audio4 = r.record(source2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4> \n",
    "    \n",
    "- Multiple transcription results may be generated by different recognition techniques within the Speech Recognition engine.\n",
    "- The Recognizer class automatically selects the result with the highest confidence score.\n",
    "- To examine other transcription results, the recognize_google function can include the show_all=True variable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_audio = r.recognize_google(audio2) + \" \" + r.recognize_google(audio3) + \" \" + r.recognize_google(audio4)\n",
    "text_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen> <b>Add punctuator</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepmultilingualpunctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepmultilingualpunctuation import PunctuationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PunctuationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_audio_punc = model.restore_punctuation(text_audio)\n",
    "text_audio_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen> <b>Recognition on non-english language</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> \n",
    "<font size = 4>\n",
    "    \n",
    "- Speech Recognition supports recognition from non-English languages.\n",
    "- In such cases, include the language=\"id-ID\" variable within the recognize_google function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = AudioSegment.from_file(file=\"/voc/work/Aldi_-_Indonesian_language_-_Bible_Verse_John_3-16.ogg\", format=\"ogg\")\n",
    "raw_file.export(out_f=\"/voc/work/Aldi_-_Indonesian_language_-_Bible_Verse_John_3-16.wav\", format=\"wav\")\n",
    "\n",
    "wav_file = AudioSegment.from_file(file=\"/voc/work/Aldi_-_Indonesian_language_-_Bible_Verse_John_3-16.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up recognizer\n",
    "r = sr.Recognizer()\n",
    "r.energy_threshold = 300\n",
    "\n",
    "# Import Audio data\n",
    "test_audio = sr.AudioFile('/voc/work/Aldi_-_Indonesian_language_-_Bible_Verse_John_3-16.wav')\n",
    "with test_audio as source:\n",
    "    r.adjust_for_ambient_noise(source)\n",
    "    audio = r.record(source)\n",
    "    \n",
    "r.recognize_google(audio, language=\"id-ID\", show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_audio_id = r.recognize_google(audio, language=\"id-ID\")\n",
    "text_audio_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#s0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 's5'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Sentiment Analysis using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "<font size = 4> \n",
    "    \n",
    "<center><b>Sentiment analysis using VADER library has already been covered in previous chapters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sid.polarity_scores(text_audio_punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sid.polarity_scores(text_audio_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
