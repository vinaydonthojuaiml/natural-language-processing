{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ThFGO3gJh6Z"
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> **Transformer Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thhvNRLLJh6b"
   },
   "source": [
    "# <center> <img src = \"https://drive.google.com/uc?export=view&id=1DXsSb3QZTO9vbI2VjJD4cpXOmj1svgSn\" height = 500 width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C1Qw3U7Jh6b"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "\n",
    "- Understand the transformer architecture\n",
    "- Explore implementation of encoder\n",
    "- Explain working of a decoder\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3lzK7BiJh6b"
   },
   "source": [
    "# <a id= 'p0'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Transformer architecture](#p1)<br>\n",
    "[2. The encoder](#p2)<br>\n",
    ">[2.1. How transformers pay attention?](#p2.1)<br>\n",
    ">[2.2. Self attention](#p2.2)<br>\n",
    ">[2.3. Calculating attention weights](#p2.3)<br>\n",
    ">[2.4. Multi-headed attention](#p2.4)<br>\n",
    ">[2.5. Feed forward layer](#p2.5)<br>\n",
    ">[2.6. Adding layer normalization](#p2.6)<br>\n",
    ">[2.7. Positional embeddings](#p2.7)<br>\n",
    ">[2.8. Add classification head](#p2.8)<br>\n",
    "\n",
    "[3. The decoder](#p3)<br>\n",
    "[4. Summary](#p4)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj_7g90-Jh6c"
   },
   "source": [
    "## <a id = 'p1'>\n",
    "<font size = 10 color = 'midnightblue'> **The transformer architecture step by step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkdiEbepJh6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "The original transformer relies on the **encoder-decoder architecture**, commonly employed in tasks such as machine translation, involving the conversion of a sequence of words from one language to another.\n",
    "\n",
    "This architecture comprises two key components:\n",
    "\n",
    "**1. Encoder:**<br>\n",
    "Converts an input sequence of tokens into a sequence of embedding vectors, often referred to as the hidden state or context.\n",
    "\n",
    "**2. Decoder** <br>\n",
    "Utilizes the hidden state from the encoder to progressively generate an output sequence of tokens, one token at a time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rlzPpd7Jh6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<center> <font size = 5> <b>The encoder and decoder are themselves composed of several building blocks.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZNihRGgJh6c"
   },
   "source": [
    "## <center> <img alt=\"transformer-encoder-decoder\" caption=\"Encoder-decoder architecture of the transformer, with the encoder shown in the upper half of the figure and the decoder in the lower half\" src=\"https://drive.google.com/uc?export=view&id=1btkUwVzTtIwAjDnndegPI4gWkfmcvYm6\" id=\"transformer-encoder-decoder\" height = 800 width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df38LPNtJh6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- <font size = 4> The input text undergoes tokenization and is converted into token embeddings.\n",
    "- <font size = 4>To address the attention mechanism's lack of awareness regarding token positions, positional embeddings containing positional information for each token are added to the token embeddings.\n",
    "\n",
    "- <font size = 4>The encoder and decoder consist of stacked layers or **blocks**, similar to stacking convolutional layers in computer vision.\n",
    "- <font size = 4>The encoder's output is fed to each decoder layer, iteratively predicting the next token until an end-of-sequence (EoS) token is reached or a maximum length is attained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qrB6ktNJh6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "<b> The Transformer architecture, initially designed for sequence-to-sequence tasks like machine translation, has evolved into three main types:</b><br>\n",
    "\n",
    "**Encoder-only:** Converts input text into a numerical representation suitable for tasks like text classification. Examples include BERT, RoBERTa, and DistilBERT, utilizing bidirectional attention.\n",
    "\n",
    "**Decoder-only:** Autocompletes sequences based on a given prompt. Models like GPT belong to this class, using causal or autoregressive attention with a focus on the left context.\n",
    "\n",
    "**Encoder-decoder:** Used for complex mappings between text sequences, suitable for tasks like machine translation. Examples include BART and T5, combining both encoder and decoder components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "En9vtSe6Jh6c"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEe7xGOXJh6d"
   },
   "source": [
    "## <a id = 'p2'>\n",
    "\n",
    "<font size = 10 color = 'midnightblue'> **The encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8sDMY_cJh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "- As it was seen earlier, the transformers' encoder consists of many encoder layers stacked next to each other.\n",
    "- As illustrated in Figure below, each encoder layer receives a sequence of embeddings and feeds them through the following sub-layers:\n",
    "\n",
    "> **1. A multi-head self-attention layer**<br>\n",
    "> **2. A fully connected feed-forward layer that is applied to each input embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11-_N0tgJh6d",
    "tags": []
   },
   "source": [
    "## <center> <img alt=\"encoder-zoom\" caption=\"Zooming into the encoder layer\" src=\"https://drive.google.com/uc?export=view&id=1oUunaulbpddoKTrscxqefR1LN-1MpEat\" id=\"encoder-zoom\" height = 400, width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_bVQnPiJh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "- <font size = 4> The output embeddings of each encoder layer have the same size as the inputs. <br>\n",
    "- The main role of the encoder stack is to <b>update</b> the input embeddings to produce representations that encode some contextual information in the sequence. <br>\n",
    "- For example, the word <b>“apple”</b> will be updated to be more <b>“company-like”</b> and less <b>“fruit-like”</b> if the words <b>“keynote”</b> or <b>“phone”</b> are close to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvgLGSk3Jh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "- **Each sublayer employs skip connections and layer normalization, common techniques for effective training of deep neural networks.**<br>\n",
    "- **To grasp the essence of a transformer's functionality, we must delve into the fundamental building block: the self-attention layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex_cgvZ6Jh6d"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4V5u2W_Jh6d"
   },
   "source": [
    "###### <a id = 'p2.1'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **How transformers pay attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U7pmGS0Jh6d"
   },
   "source": [
    "# <center> <img src = \"https://drive.google.com/uc?export=view&id=1BxdVq1Kt1NuG2Ck8mUJ8u3mk9N8TMmM5\" height = 800 width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM_I1bU5Jh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "- <font size = 4.5> An attention function can be described as mapping a query and a set of key-value pairs to an output.<br>\n",
    "- <font size = 4.5> Here the query, keys, values, and output are all vectors. <br>\n",
    "- <font size = 4.5> The output is computed as a weighted sum of the values.<br>\n",
    "- <font size = 4.5> The weight assigned to each value is computed by a compatibility function of the query with the corresponding key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqelvF-iJh6d",
    "tags": []
   },
   "source": [
    "###### <a id = 'p2.2'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Self-Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaMeOU09Jh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- <font size = 4.5>Self-attention is a mechanism that enables each element in a sequence to attend to other elements within the same sequence.<br>\n",
    "- <font size = 4.5>Introduced in the \"Attention is All You Need\" paper, self-attention allows the model to assign different weights to different elements, capturing dependencies and relationships between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytm7d0i2Jh6d"
   },
   "source": [
    "# <center> <img alt=\"Contextualized embeddings\" caption=\"Diagram showing how self-attention updates raw token embeddings (upper) into contextualized embeddings (lower) to create representations that incorporate information from the whole sequence\" src=\"https://drive.google.com/uc?export=view&id=1aqoT0tbJV-K_TvMQ025jJVdvKgXJ8iFu\" id=\"contextualized-embeddings\" height= 700 width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BprHq0iLJh6d"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "<center> <b>left:</b>  Scaled dot-product Attention\n",
    "    <b>right:</b> Multi-head attention consists of several<br>\n",
    "<center> Attention layers running in parallel. - From the paper - 'Attention is all you need'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2UMRINoJh6d"
   },
   "source": [
    "# <center> <img src = \"https://drive.google.com/uc?export=view&id=1DSlEKkHHCN4oV6NxpeiqdZmweJEqbUop\" height = 700 width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-pJ8pJBJh6d"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5O_X4WdJh6e",
    "tags": []
   },
   "source": [
    "###### <a id = 'p2.3'>\n",
    "###### <font size = 6 color = 'pwdrblue'>**Calculating attention weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCt0KdS_Jh6e"
   },
   "source": [
    "<font size = 5 color = 'seagreen'> **Scaled dot-product attention**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "Implementing a self-attention layer can be done in various ways, with scaled dot-product attention being the most common, as introduced in the original transformer architecture paper.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3sLylBZJh6e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
    "      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdvt4VdrJh6e"
   },
   "outputs": [],
   "source": [
    "# pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okE1aL26Jh6e"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- BertViz library in Jupyter is used below to visualize how attention weights are calculated in transformer models.<br>\n",
    "- BertViz, with its neuron_view module, traces the computation of weights, illustrating how query and key vectors combine to produce the final weight. <br>\n",
    "- To activate the attention visualization, instantiate the BERT checkpoint with the BertViz model class and use the show() function for a specific encoder layer and attention head. <br>\n",
    "- Click the “+” on the left to enable the attention visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2At9pSXDJh6f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "from bertviz.neuron_view import show\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "text = \"time flies like an arrow\"\n",
    "show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqwTj1VaJh6f"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "<center> Take a look at this process in more detail by implementing the diagram of operations to compute scaled dot-product attention\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doEgK9QfJh6f"
   },
   "source": [
    "# <center> <img alt=\"Operations in scaled dot-product attention\" height=\"125\" caption=\"Operations in scaled dot-product attention\" src=\"https://drive.google.com/uc?export=view&id=1wKI6spyRipODWOnY276vXXWVBeuIf61q\" id=\"attention-ops\" width = 1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8a4Q09Jh6f"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4.5>\n",
    "    \n",
    "**Note**\n",
    "- <font size = 4.5> Use PyTorch to implement the transformer architecture.<br>\n",
    "- <font size = 4.5> The steps in TensorFlow are analogous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SGpzTokJh6f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "text = \"time flies like an arrow\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFQ-c0z0Jh6f"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9UVC47sJh6f"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Each token in the sentence has been mapped to a unique ID in the tokenizer’s vocabulary. <br>\n",
    "- Also, [CLS] and [SEP] tokens have been excluded by setting <b>add_special_tokens=False</b>. <br>\n",
    "- Next, some dense embeddings are created. <br>\n",
    "- Dense in this context means that each entry in the embeddings contains a nonzero value. <br>\n",
    "- In contrast, the one-hot encodings are sparse, since all entries except one are zero. <br>\n",
    "- In PyTorch, this can be done by using a <b>torch.nn.Embedding layer</b> that acts as a lookup table for each input ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAYU9YR3Jh6f"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cwm1tkh7Jh6m"
   },
   "outputs": [],
   "source": [
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j5ciLsxJh6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4>\n",
    "\n",
    "- This has returned a tensor of shape [batch_size, seq_len, hidden_dim]. <br>\n",
    "- Postpone the positional encodings, so the next step is to create the query, key, and value vectors and calculate the attention scores using the dot product as the similarity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd77KpP-Jh6m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "query = key = value = inputs_embeds\n",
    "dim_k = key.size(-1)\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNdBSfPsJh6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "   \n",
    "<font size = 4>\n",
    "    \n",
    "- A 5X5 matrix of attention scores has been formed per sample in the batch.<br>\n",
    "- It will be observed later that the query, key, and value vectors are created by applying separate weight matrices (W(Q, K, V)) to the embeddings.<br>\n",
    "- However, for simplicity, they have been kept equal for now. <br>\n",
    "- In scaled dot-product attention, the dot products are scaled by the size of the embedding vectors to avoid excessive large numbers during training, preventing saturation of the subsequent softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nkm6iQKDJh6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "\n",
    "<b>Note:</b>\n",
    "\n",
    "- The computation of attention scores is simplified by the torch.bmm() function, which performs a batch matrix-matrix product.\n",
    "- This is applied when the query and key vectors have the shape [batch_size, seq_len, hidden_dim].\n",
    "- Ignoring the batch dimension, the dot product for each query and key vector can be calculated by transposing the key tensor to have the shape [hidden_dim, seq_len] and then the matrix product can be used to collect all dot products in a [seq_len, seq_len] matrix.\n",
    "- To perform this operation independently for all sequences in the batch, torch.bmm() is utilized, taking two batches of matrices and multiplying each matrix from the first batch with the corresponding matrix in the second batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tksewlMJh6m"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib3wCqvwJh6m"
   },
   "source": [
    "<font size = 5 color = 'seagreen'>  **The final step is to multiply the attention weights by the values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DObQcnE1Jh6m"
   },
   "outputs": [],
   "source": [
    "attn_outputs = torch.bmm(weights, value)\n",
    "attn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx2SE8OzJh6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<center><font size =4> <b>And that concludes the completion of all the steps required to implement a simplified form of self-attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzwLAp5qJh6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "<center>\n",
    "Observe that the entire process involves only two matrix multiplications and a softmax, simplifying \"self-attention\" to a sophisticated type of averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44tW3-2eJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "\n",
    "<center> <b>Now, let's encapsulate these steps into a function for future use:<\\b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4Ybi9d2Jh6n"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5es1_AhJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "- <font size = 4> In the attention mechanism with identical query and key vectors, a considerably large score is assigned to identical words in the context. <br>\n",
    "- <font size = 4> Especially to the current word itself, as the dot product of a query with itself is always 1. <br>\n",
    "- <font size = 4> However, in real-world scenarios, the meaning of a word is better enriched by complementary words in the context rather than identical ones. <br>\n",
    "- <font size = 4> For instance, the understanding of \"flies\" benefits more from information about \"time\" and \"arrow\" than from another instance of \"flies\". <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W67kJgHJh6n"
   },
   "source": [
    "<center><font size = 5 color = 'seagreen'> <b>How can we encourage this behavior?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSkB5FXFJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size = 4> **Let's enable the model to generate distinct sets of vectors for the query, key, and value of a token by utilizing three distinct linear projections.**<br>\n",
    "**These projections project our initial token vector into three different spaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZLlMPe0Jh6n"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFxRBYvhJh6n"
   },
   "source": [
    "###### <a id = 'p2.4'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Multi-headed attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIbnichoJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4>\n",
    "\n",
    "- In the example, only the embeddings for **\"as is\"** is used for attention scores and weights, but the complete process is more involved.<br>\n",
    "- In practice, the self-attention layer employs three separate linear transformations for each embedding to create query, key, and value vectors.<br>\n",
    "- These transformations project the embeddings, each with its own learnable parameters, enabling the self-attention layer to focus on diverse semantic aspects.<br>\n",
    "- It's advantageous to have multiple sets of linear projections, each representing an attention head. <br>\n",
    "- The resulting multi-head attention layer, as shown in the Figure, is crucial because the softmax of one head tends to focus on a single aspect of similarity. Multiple heads enable the model to focus on various aspects simultaneously. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q6IZhhKJh6n"
   },
   "source": [
    "<img alt=\"Multi-head attention\" height=\"125\" caption=\"Multi-head attention\" src=\"https://drive.google.com/uc?export=view&id=1meDlVWfXzPT7RQn59keoBjBFBPkMqGJU\" id=\"multihead-attention\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlMFwf-xJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "- <font size = 4>For instance, one head may concentrate on subject-verb interaction, while another identifies nearby adjectives. <br>\n",
    "- <font size = 4>These relationships are not manually crafted into the model; they are entirely learned from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTY-9PYWJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4.5> <b>\n",
    "For those acquainted with computer vision models, the resemblance to filters in convolutional neural networks becomes evident.<br></b>\n",
    "    \n",
    "<font size = 4> <center> Within such networks, one filter may identify faces, while another detects wheels of cars in images.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7Ri9QXUJh6n"
   },
   "source": [
    "<center><font size = 5 color = 'seagreen'> <b>Let’s implement this layer by first coding up a single attention head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsGsWj7UJh6n"
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        attn_outputs = scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi_TC57eJh6n"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- <font size = 4> Here, three separate linear layers are initialized.\n",
    "- <font size = 4> Matrix multiplication is performed on the embedding vectors to yield tensors of shape [batch_size, seq_len, head_dim].<br>\n",
    "- <font size = 4> Here, head_dim represents the dimensions being projected into.<br>\n",
    "- <font size =4> While head_dim is not required to be smaller than the number of embedding dimensions (embed_dim) of the tokens, it's typically chosen as a multiple of embed_dim for consistent computation across each head.<br>\n",
    "<font size =4>**For instance, BERT utilizes 12 attention heads, resulting in a dimension of each head being 768/12 = 64**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tONq99eMJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> Now, with a singular attention head established, we can merge the outputs of each one to construct the complete multi-head attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU3RkBsuJh6o"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSeD71dXJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "\n",
    "- Observe that the concatenated output from the attention heads undergoes processing through a concluding linear layer, yielding an output tensor of shape **_[batch_size, seq_len, hidden_dim]_**, apt for the subsequent feed-forward network. <br>\n",
    "- To verify, the **_MultiHeadAttention_** module is initialized with the configuration loaded earlier from the pre-trained BERT model, guaranteeing alignment with BERT settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7wOrXXDJh6o"
   },
   "outputs": [],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJOp7xUhJh6o"
   },
   "source": [
    "<center><font size = 5 color = 'seagreen'> <b>It functions successfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8TC1BwMJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**To conclude:**\n",
    "- BertViz is employed once more to visualize the attention for two instances of the word \"flies.\" <br>\n",
    "- The **_head_view()_** function from BertViz is applied, computing attentions from a pre-trained checkpoint and indicating the sentence boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Re0scDOJh6o"
   },
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
    "\n",
    "sentence_a = \"time flies like an arrow\"\n",
    "sentence_b = \"fruit flies like a banana\"\n",
    "\n",
    "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
    "attention = model(**viz_inputs).attentions\n",
    "sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n",
    "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
    "\n",
    "head_view(attention, tokens, sentence_b_start, heads=[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfQBgiUBJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4>\n",
    "\n",
    "- The visualization depicts attention weights as lines connecting the token undergoing embedding update (left) with each attended word (right). <br>\n",
    "- Line intensity signifies attention weight strength, with dark lines indicating values close to 1 and faint lines representing values close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cI-hEaxJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4> **In this example, the input consists of two sentences and the [CLS] and [SEP] tokens are the special tokens in BERT’s tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2UtWpLTJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "- From the visualization, we observe the strongest attention weights between words within the same sentence, indicating BERT's ability to recognize intra-sentence connections.<br>\n",
    "- Specifically for the word \"flies,\" BERT emphasizes the importance of \"arrow\" in the first sentence and \"fruit\" and \"banana\" in the second. <br>\n",
    "- These attention weights enable the model to discern whether \"flies\" is used as a verb or noun, depending on its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEBqdakYJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>Having explored attention, let's delve into implementing the remaining component of the encoder layer:\n",
    "\n",
    "**position-wise feed-forward networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7ZwzOm3Jh6o"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVdSbBXXJh6o",
    "tags": []
   },
   "source": [
    "###### <a id = 'p2.5'>\n",
    "###### <font size = 6 color = 'pwdrblue'>  **The feed-forward layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR05re7KJh6o"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "- <font size = 4> The encoder and decoder's feed-forward sublayer consist of a straightforward two-layer fully connected neural network, with a distinctive feature:<br><ind>\n",
    "- <font size = 4>It processes each embedding independently, earning it the label of a position-wise feed-forward layer. <br>\n",
    "- <font size = 4>Alternatively, individuals with a background in computer vision may refer to it as a one-dimensional convolution with a kernel size of one (as seen in the OpenAI GPT codebase). <br>\n",
    "- <font size =4>A common guideline suggests setting the hidden size of the first layer to four times the size of the embeddings, and the GELU activation function is often applied.\n",
    "-<font size = 4>This layer is considered crucial for capacity and memorization, frequently adjusted when scaling up models. Implementation as a simple nn.Module can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrgBxr7nJh6o"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9cV5GljJh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Note:**\n",
    "\n",
    "- It's worth noting that a feed-forward layer, like <b>nn.Linear</b>, is typically employed on a tensor with a shape of (batch_size, input_dim), operating independently on each element within the batch dimension. <br>\n",
    "- This holds true for any dimension except the last one. Therefore, when presenting a tensor with a shape of (batch_size, seq_len, hidden_dim), the layer processes all token embeddings of the batch and sequence independently, aligning with our intended behavior. <br>\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<center><font size = 4> <b>We can confirm this by passing the attention outputs:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTpX-tMHJh6p"
   },
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_outputs)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxIY9KcBJh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "<b>\n",
    "    \n",
    "- All the necessary elements are now in place to construct a complete transformer encoder layer!<br>\n",
    "- The remaining decision is the placement of skip connections and layer normalization.\n",
    "\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "<b>\n",
    "    <center> <font color = 'seagreen'> Let's explore how this choice influences the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVH6cr5fJh6p"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_QdddArJh6p"
   },
   "source": [
    "###### <a id = 'p2.6'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Adding Layer Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r06BuUMAJh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4> - The Transformer architecture incorporates layer normalization and skip connections.\n",
    "\n",
    "<font size = 4> - Layer normalization ensures each input in the batch has zero mean and unity variance.\n",
    "    \n",
    "<font size = 4> - Skip connections transmit a tensor to the next layer without processing and add it to the processed tensor.\n",
    "    \n",
    "<font size = 4> - Regarding the placement of layer normalization in Transformer's encoder or decoder layers, two main choices exist in the literature:\n",
    "    \n",
    "> <font size = 4><b>a. Post Layer Normalization:</b>\n",
    "    \n",
    "> * <font size = 4>Layer normalization is positioned between the skip connections.<br>\n",
    "> * <font size = 4>Training from scratch with this arrangement can be challenging due to potential gradient divergence.<br>\n",
    "> * <font size = 4>To address this, a concept called learning rate warm-up is often applied, gradually increasing the learning rate during training.<br>\n",
    "    \n",
    "\n",
    "> <font size = 4><b>b. Pre Layer Normalization:\n",
    "> * <font size = 4>Layer normalization is situated within the span of the skip connections.<br>\n",
    "> * <font size = 4>This configuration is more stable during training and typically does not require learning rate warm-up.<br>\n",
    "\n",
    "<font size = 4><b>The figure below illustrates the difference between these two arrangements.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMFLOeHBJh6p"
   },
   "source": [
    "# <center> <img alt=\"Transformer layer normalization\" height=\"500\" caption=\"Different arrangements of layer normalization in a transformer encoder layer\" src=\"https://drive.google.com/uc?export=view&id=1qNT-xpVik7gjM5x-4s9C9he8KrFPURph\" id=\"layer-norm\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL0WYwKMJh6p"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Use the second arrangement, so we can simply stick together our building blocks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Kzatn5-Jh6p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer normalization and then copy input into query, key, value\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00mbM_MzJh6p"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Testing this with our input embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYAr0Ge8Jh6p"
   },
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "inputs_embeds.shape, encoder_layer(inputs_embeds).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpOTX5OqJh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "- <font size = 4>The initial implementation of the transformer encoder layer is complete.<br>\n",
    "- <font size = 4>However, there is a limitation in the way the encoder layers are configured: <br>\n",
    "    > <font size = 4>**They are entirely invariant to the position of the tokens.** <br>\n",
    "- <font size = 4>As the multi-head attention layer operates as a sophisticated weighted sum, it results in the loss of information regarding token positions. <br>\n",
    "> <font size = 4> **Fortunately, there's a straightforward solution to incorporate positional information by utilizing positional embeddings.**\n",
    "    <font size = 4> <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIxeut-bJh6p"
   },
   "source": [
    "###### <a id = 'p2.7'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Positional embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni9QaSo_Jh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4> **Positional embeddings employ a straightforward yet impactful concept:**\n",
    "  \n",
    ">* <font size = 4>They enhance token embeddings with a position-dependent pattern arranged in a vector. <br>\n",
    ">* <font size = 4>By ensuring a distinct pattern for each position, attention heads and feed-forward layers in each stack can learn to integrate positional information into their transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSpy3YkCJh6p"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- <font size =4>A prevalent approach involves using a learnable pattern, particularly with a substantial pre-training dataset.\n",
    "- <font size =4>This mirrors token embeddings but utilizes the position index instead of the token ID as input.\n",
    "- <font size =4>This approach efficiently learns to encode token positions during pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc-ToCNdJh6q"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Let's craft a custom Embeddings module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eDS_bnRJh6q"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Combining a token embedding layer projecting input_ids to a dense hidden state\n",
    "- With a positional embedding doing the same for position_ids\n",
    "- The resulting embedding is a simple sum of both embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIoQ6l3kJh6q"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2v25UpyJh6q"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embeddings(config)\n",
    "embedding_layer(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xC_MNobJh6q"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "**Observations:**\n",
    "- The embedding layer now generates a singular, dense embedding per token.\n",
    "- While widely used, alternatives to learnable position embeddings include:\n",
    "- Absolute positional representations: Employ static sine and cosine signal patterns to encode token positions, effective with limited data.\n",
    "- Relative positional representations: Prioritize surrounding tokens during embedding computation, considering the relative positions between tokens. This requires modifying the attention mechanism to incorporate relative position terms, as seen in models like DeBERTa.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C_D44ZGJh6q"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Now, let's integrate these components to construct the complete transformer encoder, combining embeddings with encoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWAOoyQ1Jh6q"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvDg2luAJh6q"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Check the output shapes of the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG6x9UEqJh6q"
   },
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zoCG2ZZJh6q"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "- <font size =4> A hidden state is obtained for each token in the batch.<br>\n",
    "- <font size =4> This offers flexibility for diverse applications such as predicting missing tokens in masked language modeling or determining the start and end position of an answer in question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB8_TAKpJh6q"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUdP2BTLJh6q"
   },
   "source": [
    "###### <a id = 'p2.8'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Adding a classification head**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrKJEtOqJh6q"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4>\n",
    "\n",
    "- The Transformer model comprises a <b>task-independent body</b> and a <b>task-specific head</b>.<br>\n",
    "    \n",
    "> The current development focuses on the body. <br>\n",
    "    \n",
    "- <font size = 4> To construct a text classifier, append a classification head to this body. <br>\n",
    "- <font size = 4> Utilize the hidden states for each token, various approaches exist. <br>\n",
    "- <font size = 4> Typically, the initial token is employed for prediction, incorporating dropout and a linear layer.<br>\n",
    "- <font size = 4> The subsequent class extends the existing encoder for sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHde94BsJh6q"
   },
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76TgmG81Jh6q"
   },
   "source": [
    "<font size = 5 color = seagreen>\n",
    "<b>\n",
    "    <center> Define the number of classes we would like to predict before initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDzTFZulJh6q"
   },
   "outputs": [],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4egWDgeJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    "<center> <b>This is precisely what we've sought.</b>\n",
    "    \n",
    "\n",
    "Unnormalized logits for each class in the output are obtained for every example in the batch.<br>\n",
    "With this, our examination of the encoder and its integration with a task-specific head is concluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFOyrHuEJh6r"
   },
   "source": [
    "### <center> **Lets talk about decoders.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRSsT-FFJh6r"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FThGRhnXJh6r"
   },
   "source": [
    "## <a id = 'p3'>\n",
    "\n",
    "<font size = 10 color = 'midnightblue'> **The Decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pahKdjNJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4> In the illustration, the decoder, distinct from the encoder, incorporates two attention sublayers:\n",
    "\n",
    "<font size = 4>\n",
    "    \n",
    "> **1. Masked multi-head self-attention layer:**<br>\n",
    "    > - Ensures generation of tokens at each timestep is solely based on past outputs and the current predicted token. <br>\n",
    "    > - This prevents the decoder from trivially copying target translations during training.<br>\n",
    "\n",
    "> **2. Encoder-decoder attention layer:**<br>\n",
    "    > - Conducts multi-head attention over the encoder stack's output key and value vectors, using the intermediate representations of the decoder as queries.\n",
    "    > - This enables learning the relationship between tokens from different sequences, such as different languages.\n",
    "    > - The decoder accesses encoder keys and values in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5xj12rMJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size = 4>\n",
    "    \n",
    "> * The necessary modifications for masking in the self-attention layer will be explored.\n",
    "> * While the implementation of the encoder-decoder attention layer is left as an exercise.\n",
    "> * The masked self-attention involves introducing a mask matrix with ones on the lower diagonal and zeros above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jlJjYaBJh6r"
   },
   "source": [
    "# <center> <img alt=\"Transformer decoder zoom\" caption=\"Zooming into the transformer decoder layer\" src=\"https://drive.google.com/uc?export=view&id=1Dmk3n1nkQoiHPlDBYyQfc4pvV20I4X47\" id=\"decoder-zoom\" width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbOFsNFhJh6r"
   },
   "outputs": [],
   "source": [
    "seq_len = inputs.input_ids.size(-1)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n",
    "mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WkpnKLpJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <font size = 4> <b>Here,</b><br\n",
    "\n",
    "> - PyTorch's tril() function is employed to generate the lower triangular matrix. <br>\n",
    "> - With this mask matrix, each attention head can be restricted from observing future tokens by utilizing Tensor.masked_fill() to replace zeros with negative infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdpfysbdJh6r"
   },
   "outputs": [],
   "source": [
    "scores.masked_fill(mask == 0, -float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wpa0ZOFLJh6r"
   },
   "source": [
    "# <center> <img src = \"https://drive.google.com/uc?export=view&id=1xQXW0Wp7cVRisDiwpgJBDpVNNFmjc2FM\" height = 1100 width = 800 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnuvgGNoJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> **By assigning negative infinity to the upper values,**\n",
    "    \n",
    "<font size = 4>\n",
    "    \n",
    "> - It is ensured that the attention weights become zero after applying softmax to the scores.\n",
    "> - This masking behavior can be incorporated with a slight modification to our previously implemented scaled dot-product attention function in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEyhyLSBJh6r"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q52x3T0xJh6r"
   },
   "source": [
    "<a id = 'p4'>\n",
    "\n",
    "<font size = 10 color = 'midnightblue'> **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9yEN0qGJh6r"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> **In this notebook, three primary transformer model architectures have been presented:**\n",
    "\n",
    "<font size = 4>\n",
    "    \n",
    "> <b>1. encoders, <br>\n",
    "2. decoders, and <br>\n",
    "3. encoder-decoders. </b>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "    \n",
    "* The success of early transformer models sparked extensive development.\n",
    "* With researchers creating models for various datasets, utilizing different pre-training objectives, and adjusting architectures for enhanced performance.\n",
    "* Despite the ongoing expansion of model diversity, they can broadly be categorized into these three groups.\n",
    "    </div>\n",
    "    <font size = 4> <b>The diagram below highlights a few of the architectural milestones.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSivWL2cJh6r"
   },
   "source": [
    "## <center> <img alt=\"Transformer family tree\" caption=\"An overview of some of the most prominent transformer architectures\" src=\"https://drive.google.com/uc?export=view&id=18G8p9uJMud3QHo2X5dDpn0WGczcMAd79\" id=\"family-tree\" width = 700 height = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUQQeKOIJh6s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
