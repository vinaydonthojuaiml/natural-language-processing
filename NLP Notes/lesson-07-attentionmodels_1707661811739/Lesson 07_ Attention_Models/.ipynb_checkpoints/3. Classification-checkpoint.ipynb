{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> **Classification -  Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <center> <img src = \"https://drive.google.com/uc?export=view&id=16oN7Hn9yxnFnMrwGzTIzfMOELv4lNC-J\" height = 500, width = 1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "- Learn to perform classification using transformers\n",
    "- Understand fine-tuning and using a pre-trained models\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id= 'p0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Introduction](#p1)<br>\n",
    "[2. Load the dataset](#p2)<br>\n",
    "[3. Explore and prepare data](#p3)<br>\n",
    "[4. Training the classifier](#p4)\n",
    "> [4.1. Feature extraction](#p4.1)<br>\n",
    "> [4.2. Extracting the last hidden states](#p4.2)<br>\n",
    "> [4.3. Creating featurematrix](#p4.3)<br>\n",
    "> [4.4. Visualizing the training set](#p4.4)<br>\n",
    "> [4.5. Training a simple classifier](#p4.5)<br>\n",
    "\n",
    "[5. Fine-tuning Transformers](#p5)<br>\n",
    "[6. Loading a pre-trained model](#p6)<br>\n",
    "    \n",
    "> [6.1. Defining the performance metrics](#p6.1)<br>\n",
    "> [6.2 Training the model](#p6.2)<br>   \n",
    "    \n",
    "[7. Error analysis](#p7)<br> \n",
    "[8. Saving and sharing the model ](#p15)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p1'>\n",
    "    \n",
    "<font size = 10 color = 'midnightblue'>  **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>  \n",
    "        \n",
    "* The key benefit of this model is that it performs similarly to BERT but is much smaller and more efficient.<br>\n",
    "* This means we can quickly train a classifier in just a few minutes. <br>\n",
    "* If a preference is for a larger BERT model, transitioning to the checkpoint of the pre-trained model can be accomplished effortlessly.<br>\n",
    "* A checkpoint refers to the set of weights loaded into a specific transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> \n",
    "\n",
    "**A typical pipeline for training transformer models with the datasets, tokenizers, and  transformers libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Hugging Face Pipeline\" caption=\"A typical pipeline for training transformer models with the image:images/logo.png[hf,13,13] Datasets, image:images/logo.png[hf,13,13] Tokenizers, and image:images/logo.png[hf,13,13] Transformers libraries\" src=\"https://drive.google.com/uc?export=view&id=1NO06f76MUxksiH3Q9F2vpk5SNAGZtjQd\" id=\"hf-libraries\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id ='p2'>\n",
    "<font size = 10 color = 'midnightblue'>  <b>The Dataset</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size =4>\n",
    "\n",
    "* To create our emotion detector, utilize a valuable dataset from an article investigating how emotions manifest in english twitter messages. \n",
    "\n",
    "*  Unlike typical sentiment analysis datasets that only involve \"positive\" and \"negative\" polarities, this dataset encompasses six fundamental emotions: **anger, disgust, fear, joy, sadness, and surprise**. \n",
    "\n",
    "The objective is to train a model that, when given a tweet, can effectively classify it into one of these distinct emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
    "print(f\"The first 10 are: {all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 5>\n",
    "    \n",
    "Each dataset is given a name, load the `emotion` dataset with the function `load_dataset()` function:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 5> \n",
    "    \n",
    "**Looking inside the `emotions` object:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**Observations:** \n",
    "> * It resembles a Python dictionary, where each key corresponds to a distinct split. \n",
    "> * The typical dictionary syntax can be used to access a specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = emotions[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- <font size = 4> This yields an instance of the Dataset class. \n",
    "- <font size = 4> The Dataset object stands as a fundamental data structure in Hugging Faces Datasets.\n",
    "- <font size = 4> Initially, it functions akin to a typical Python array or list, allowing us to inquire about its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **or access a single example by its index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'>**Here we see that a single row is represented as a dictionary, where the keys correspond to the column names:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- <font size =4> The keys represent the tweet and the corresponding emotion. \n",
    "- <font size =4> This mirrors the reliance on **Apache Arrow** in Datasets, which establishes a typed columnar format.\n",
    "- <font size =4> This format is more memory-efficient than native Python. \n",
    "- <font size =4> To discern the underlying data types being employed, we can access the features attribute of a Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "In this case, the data type of the `text` column is `string`, while the `label` column is a special `ClassLabel` object that contains information about the class names and their mapping to integers. <br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Several rows can also be accessed with a slice:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 5> \n",
    "    \n",
    "**Note that in this case, the dictionary values are now lists instead of individual elements.** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> <center>**Get the full column by name:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_ds[\"text\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "    <b>Bonus learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen' >**Loading data which is not on the Hub?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src =\"https://drive.google.com/uc?export=view&id=1RUmXI8qEjlPaK4skT1vXPKfnYcBQEjF8\" height = 700 width = 900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt\"\n",
    "!wget {dataset_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    ">- It can be seen that here are no column headers and each tweet and emotion are separated by a semicolon. \n",
    ">- Nevertheless, this is quite similar to a CSV file, so we can load the dataset locally by using the `csv` script and pointing the `data_files` argument to the _train.txt_ file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_local = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", \n",
    "                              names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt\"\n",
    "emotions_remote = load_dataset(\"csv\", data_files=dataset_url, sep=\";\", \n",
    "                               names=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "<b>Bonus learning corner ends</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p3'>\n",
    "    \n",
    "<font size = 10 color = 'midnightblue'>   **Explore and prepare the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> <b>Change Datasets to DataFrames</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Column headers are maintained, and the initial rows align with our prior data views.\n",
    "- Labels are currently in integer format.\n",
    "- Utilize the `int2str()` method of the `label` feature.\n",
    "- Create a new column in the `DataFrame` with the label names.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Class Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4> **How Long Are Our Tweets?**\n",
    "\n",
    "\n",
    "<font size = 4>\n",
    "- Transformer models come with a maximum input sequence length known as the \"maximum context size.\" <br>\n",
    "- When working with DistilBERT, the maximum context size is set at 512 tokens, equivalent to a few paragraphs of text.<br>\n",
    "- In the upcoming section, we'll delve into tokens, considering them as atomic text pieces for now, treated as individual words. <br>\n",
    "- To estimate tweet lengths per emotion, we can examine the distribution of words per tweet.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False,\n",
    "           color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    "- The plot reveals that, for each emotion, the majority of tweets are approximately 15 words in length, well within DistilBERT's maximum context size.\n",
    "- Texts exceeding a model's context size require truncation, potentially compromising performance if vital information is lost. \n",
    "- Fortunately, based on this observation, it seems that won't be a concern in this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4> <b> <center> Converting these raw texts into a format suitable for Transformers! </b>\n",
    "<font size = 4> <center> Also reset the output format of the dataset since the `DataFrame` format is not needed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'><b>Text to Tokens</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size = 4>\n",
    "- Transformer models such as DistilBERT don't accept raw strings as input.<br>\n",
    "- Instead, they expect text to be tokenized and encoded as numerical vectors. <br>\n",
    "- Tokenization involves breaking down a string into the atomic units used by the model. <br>\n",
    "- Various tokenization strategies exist, with the optimal splitting of words into subunits typically learned from the corpus. <br>\n",
    "\n",
    "    \n",
    "<center> <b>Before examining DistilBERT's tokenizer, let's explore two extreme cases: character and word tokenization.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Character Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4> \n",
    "    \n",
    "- The simplest tokenization scheme is to feed each character individually to the model. <br>\n",
    "- In Python, `str` objects are really arrays under the hood, which allows us to quickly implement character-level tokenization with just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <font size =4> <b><center>This is a good start, but its not done yet. </b>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- The model expects each character to be  converted to an integer, a process sometimes called `_numericalization_`. \n",
    "- One simple way to do this is by encoding each unique token (which are characters in this case) with a unique integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- This gives us a mapping from each character in our vocabulary to a unique integer. \n",
    "- Use `token2idx` to transform the tokenized text to a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- Each token has now been mapped to a unique numerical identifier (hence the name `input_ids`). \n",
    "- The last step is to convert `input_ids` to a 2D tensor of one-hot vectors. \n",
    "- One-hot vectors are frequently used in machine learning to encode categorical data, which can be either ordinal or nominal. \n",
    "\n",
    "    For example, suppose we wanted to encode the names of characters in the _Transformers_ TV series. One way to do this would be to map each name to a unique ID, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_df = pd.DataFrame(\n",
    "    {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\n",
    "categorical_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>    \n",
    "    \n",
    "- The problem with this approach is that it creates a fictitious ordering between the names, and neural networks are _really_ good at learning these kinds of relationships. \n",
    "- So instead, we can create a new column for each category and assign a 1 where the category is true, and a 0 otherwise. \n",
    "- In Pandas, this can be implemented with the `get_dummies()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(categorical_df[\"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- The rows of this `DataFrame` are the one-hot vectors, which have a single \"hot\" entry with a 1 and 0s everywhere else.\n",
    "- Now, looking at the `input_ids`, there is a similar problem: \n",
    ">* The elements create an ordinal scale. \n",
    ">* This means that adding or subtracting two IDs is a meaningless operation, since the result is a new ID that represents another random token.\n",
    "\n",
    "- On the other hand, the result of adding two one-hot encodings can easily be interpreted: \n",
    "> * The two entries that are \"hot\" indicate that the corresponding tokens co-occur. \n",
    "> * Create the one-hot encodings in PyTorch by converting `input_ids` to a tensor and applying the `one_hot()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size=4> **For each of the 38 input tokens we now have a one-hot vector with 20 dimensions, since our vocabulary consists of 20 unique characters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<font size =4>\n",
    "    \n",
    "> **Warning**: \n",
    "    \n",
    "> - It's important to always set `num_classes` in the `one_hot()` function.\n",
    "> - This is because otherwise the one-hot vectors may end up being shorter than the length of the vocabulary (and need to be padded with zeros manually). \n",
    "> - In TensorFlow, the equivalent function is `tf.one_hot()`, where the `depth` argument plays the role of `num_classes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**By examining the first vector, we can verify that a 1 appears in the location indicated by `input_ids[0]`:**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- From the simple example we can see that character-level tokenization ignores any structure in the text and treats the whole string as a stream of characters.\n",
    "- Although this helps deal with misspellings and rare words, the main drawback is that linguistic structures such as words need to be _learned_ from the data. \n",
    "- This requires significant compute, memory, and data. \n",
    "    <center> <b>For this reason, character tokenization is rarely used in practice. </b>\n",
    "- Instead, some structure of the text is preserved during the tokenization step.\n",
    "- `_Word tokenization_` is a straightforward approach to achieve this, so let's take a look at how it works.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Word Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "  \n",
    "- Instead of splitting the text into characters, we can split it into words and map each word to an integer. \n",
    "- Using words from the outset enables the model to skip the step of learning words from characters, and thereby reduces the complexity of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- One simple class of word tokenizers uses whitespace to tokenize the text. \n",
    "- This can be done by applying Python's `split()` function directly on the raw text (just like we did to measure the tweet lengths):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Take the same steps we took for the character tokenizer to map each word to an ID. \n",
    "- However, we can already see one potential problem with this tokenization scheme: \n",
    "> **Punctuation is not accounted for, so `NLP.` is treated as a single token.** \n",
    "\n",
    "<center><b>Given that words can include declinations, conjugations, or misspellings, the size of the vocabulary can easily grow into the millions!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "    \n",
    "<b>Note</b>: \n",
    "    <font size = 4>\n",
    "        \n",
    "> - Some word tokenizers have extra rules for punctuation. <br>\n",
    "> - One can also apply stemming or lemmatization, which normalizes words to their stem.<br>\n",
    "> - For e.g. \"great\", \"greater\", and \"greatest\" all become \"great\".<br>\n",
    "> - This normalization comes at the expense of losing some information in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Having a large vocabulary is a problem because it requires neural networks to have an enormous number of parameters. \n",
    "- To illustrate this, suppose we have 1 million unique words and want to compress the 1-million-dimensional input vectors to 1-thousand-dimensional vectors in the first layer of our neural network. \n",
    "- This is a standard step in most NLP architectures, and the resulting weight matrix of this first layer would contain 1 million $\\times$ 1 thousand = 1 billion weights. \n",
    "- This is already comparable to the largest GPT-2 model, which has around 1.5 billion parameters in total!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- The goal is to reduce model parameter waste, given the high training cost and increased maintenance complexity associated with larger models.\n",
    "\n",
    "- A common strategy involves restricting the vocabulary to, for instance, the top 100,000 words in the corpus, discarding rare words.\n",
    "\n",
    "- Words outside the vocabulary are labeled as \"unknown\" and mapped to a shared UNK token.\n",
    "\n",
    "- This results in the loss of potentially valuable information during word tokenization, as the model lacks information about words associated with UNK.\n",
    "\n",
    "Isn't it desirable to find a middle ground between character and word tokenization that preserves both input information and structure? Enter: subword tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Subword Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "- Subword tokenization combines the strengths of character and word tokenization.\n",
    ">- It breaks down rare words into smaller units for handling complexity and misspellings.\n",
    ">- It retains frequent words as distinct entities to manage input length effectively.\n",
    ">- The process of subword tokenization is learned from the pretraining corpus.\n",
    ">- It involves a combination of statistical rules and algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "**Various subword tokenization algorithms are commonly used in NLP.**\n",
    ">- One example is WordPiece, utilized by BERT and DistilBERT tokenizers.\n",
    ">- Transformers offers a convenient `AutoTokenizer` class for quick loading of tokenizers.\n",
    ">- The `from_pretrained()` method, specifying the model ID, facilitates easy loading.\n",
    ">- DistilBERT's tokenizer can be loaded effortlessly using these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- The `AutoTokenizer` class is part of a broader collection of \"auto\" classes.\n",
    "- Its purpose is automatic retrieval of the model's configuration, pre-trained weights, or vocabulary based on the checkpoint's name.\n",
    "- This feature facilitates swift switching between models.\n",
    "- If desired, manual loading of the specific class is also possible.\n",
    "- For instance, the DistilBERT tokenizer can be loaded manually using a the following method.\n",
    "\n",
    "```python\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "\n",
    "**Note:** \n",
    "> - When the method `AutoTokenizer.from_pretrained()` is run for the first time a progress bar can be seen.\n",
    "> - It shows which parameters of the pretrained tokenizer are loaded from the Hugging Face Hub. \n",
    "> - When the code is executed a second time, it will load the tokenizer from the cache, usually located at _~/.cache/huggingface/_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> <center> <b>Let's examine how this tokenizer works by feeding it our simple \"Tokenizing text is a core task of NLP.\" example text:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Just like with the character tokenization, it can be seen that the words have been mapped to unique integers in the `input_ids` field.\n",
    "- The role of the `attention_mask` field will be discussed in the next section. \n",
    "\n",
    "Now that the `input_ids` are there, they can be converted back into tokens by using the tokenizer's `convert_ids_to_tokens()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "\n",
    "<b>Observations: </b>\n",
    "     \n",
    "> 1. Some special `[CLS]` and `[SEP]` tokens have been added to the start and end of the sequence. <br>\n",
    "    - These tokens differ from model to model, but their main role is to indicate the start and end of a sequence. \n",
    "\n",
    "> 2. The tokens have each been lowercased, which is a feature of this particular checkpoint. \n",
    "\n",
    "> 3. Finally, we can see that \"tokenizing\" and \"NLP\" have been split into two tokens, which makes sense since they are not common words. \n",
    "    - The `##` prefix in `##izing` and `##p` means that the preceding string is not whitespace; any token with this prefix should be merged with the previous token when you convert the tokens back to a string.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> <b>The `AutoTokenizer` class has a `convert_tokens_to_string()` method for doing just that, so let's apply it to our tokens:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<font size = 5 color = 'seagreen'> **The `AutoTokenizer` class also has several attributes that provide information about the tokenizer.**<br>\n",
    "<font size = 5 color = 'seagreen'> **For example, we can inspect the vocabulary size:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **and the corresponding model's maximum context size:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Another interesting attribute to know about is the names of the fields that the model expects in its forward pass:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4> <b>Now that we have a basic understanding of the tokenization process for a single string, let's see how we can tokenize the whole dataset!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<font size =4>\n",
    "\n",
    "**Warning**: \n",
    "    \n",
    ">- When using pre-trained models, it is _really_ important to make sure that you use the same tokenizer that the model was trained with. <br>\n",
    ">- From the model's perspective, switching the tokenizer is like shuffling the vocabulary. <br>\n",
    ">- If everyone started swapping random words like \"house\" for \"cat,\" it will be very hard to understand what was going on too!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Tokenizing the Whole Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size =4>\n",
    "    \n",
    "- To tokenize the whole corpus, the `map()` method of the `DatasetDict` object can be used.\n",
    "\n",
    "- To get started, the first thing we need is a processing function to tokenize our examples with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- This function applies the tokenizer to a batch of examples; \n",
    "- `padding=True` will pad the examples with zeros to the size of the longest one in a batch, and `truncation=True` will truncate the examples to the model's maximum context size. \n",
    "- To see `tokenize()` in action, pass a batch of two examples from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "    \n",
    "**The consequence of padding can be observed here.**\n",
    ">- The initial element of input_ids is shorter than the second.\n",
    ">- Zeros have been appended to the first element to equalize their lengths.\n",
    ">- These zeros correspond to the `[PAD]` token in the vocabulary.\n",
    ">- The set of special tokens encompasses not only [PAD] but also includes `[CLS]` and `[SEP]`, as introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "tokens2ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\n",
    "data = sorted(tokens2ids, key=lambda x : x[-1])\n",
    "df = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- The tokenizer, in addition to returning encoded tweets as input_ids, provides a list of attention_mask arrays.\n",
    "- This inclusion is aimed at preventing model confusion caused by extra padding tokens.\n",
    "- The purpose of the attention mask is to guide the model in ignoring the padded segments of the input.\n",
    "- For a visual explanation of the padding of input IDs and attention masks, refer to the diagram labeled <<attention-mask>>.\n",
    "</div>\n",
    "<center> <img alt=\"attention-mask\" src=\"https://drive.google.com/uc?export=view&id=1p0r4gtzs_Ecj_Qtg4iu8eyu35QNxOZE2\" id=\"attention-mask\" width = 900> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "**For each batch, the input sequences are padded to the maximum sequence length in the batch; the attention mask is used in the model to ignore the padded areas of the input tensors**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "**Once we've defined a processing function, we can apply it across all the splits in the corpus in a single line of code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "\n",
    "- By default, the `map()` method processes each example in the corpus individually.\n",
    "- Setting `batched=True` in the `map()` method allows encoding tweets in batches.\n",
    "- With `batch_size=None`, the `tokenize()` function is applied to the entire dataset as a single batch.\n",
    "- This ensures uniform shapes for input tensors and attention masks across the entire dataset.\n",
    "- The operation adds new `input_ids` and `attention_mask` columns to the dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"p4\">\n",
    "<font size = 10 color = 'midnightblue'>\n",
    "    \n",
    "**Training a text classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "- As previously mentioned, models such as DistilBERT are pre-trained to predict masked words in a text sequence.\n",
    "- However, direct use of these language models for text classification requires some modifications.\n",
    "- To grasp the necessary modifications, let's examine the architecture of an encoder-based model like DistilBERT.\n",
    "- The architecture depiction will help us understand the adjustments needed for text classification.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img alt=\"encoder-classifier\" src=\"https://drive.google.com/uc?export=view&id=14MZpmcbPCnJvDGYn9e_Bzw4HPq8Axrdn\" width = 700 height = 800 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Initial step involves tokenization of text, generating one-hot vectors known as token encodings.\n",
    "- The dimension of token encodings is determined by the tokenizer vocabulary, typically ranging from 20k to 200k unique tokens.\n",
    "- Subsequently, these token encodings transform into token embeddings, residing in a lower-dimensional space.\n",
    "- The token embeddings then undergo processing through encoder block layers, resulting in a hidden state for each input token.\n",
    "- In language modeling pre-training, such as in DistilBERT, the hidden states are used to predict masked input tokens through a dedicated layer.\n",
    "- In the context of a classification task, the language modeling layer is substituted with a classification layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "\n",
    "**Note:**\n",
    "> - In practical implementation, PyTorch omits the creation of one-hot vectors for token encodings.\n",
    "> - This is because multiplying a matrix by a one-hot vector is equivalent to selecting a column from the matrix.\n",
    "> - The direct method involves retrieving the column with the token ID from the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "The model can be trained on the Twitter dataset using two approaches:\n",
    "\n",
    "- Feature extraction: Utilizing the hidden states as features, we train a classifier without altering the pre-trained model.\n",
    "- Fine-tuning: Training the entire model end-to-end, updating parameters of the pre-trained model in the process.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p4.1'> \n",
    "###### <font size = 6 color = 'pwdrblue'> **Feature extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =5 color = 'seagreen'> **Transformer as a feature extractor** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Employing a transformer as a feature extractor is straightforward.\n",
    "- During training, we keep the body's weights frozen and utilize the hidden states as features for the classifier.\n",
    "- This approach offers the benefit of swift training for a compact or shallow model.\n",
    "- The model could be a neural classification layer or a gradient-independent method like a random forest.\n",
    "- This method proves particularly advantageous in the absence of GPUs, as the hidden states require pre-computation only once.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> <img src=\"https://drive.google.com/uc?export=view&id=14itYn1bhOflvKCfErUk5bzMkTsWRPknT\" width = 800>\n",
    "    \n",
    "<font size = 3> <center> **The feature-based approach involves freezing the DistilBERT model, utilizing it solely to generate features for a classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =5 color = 'seagreen'> **Using pretrained models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "- Another convenient auto class from Transformers, known as `AutoModel`, will be utilized. \n",
    "- Similar to the AutoTokenizer class, the `from_pretrained()` method is available in `AutoModel` for loading the weights of a pre-trained model. \n",
    "- This method will be employed to load the DistilBERT checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- Using PyTorch, the availability of a GPU is verified.\n",
    "- The PyTorch `nn.Module.to()` method is sequentially applied to the model loader.\n",
    "- This guarantees the model's execution on the GPU if available.\n",
    "- If a GPU is not present, the model automatically runs on the CPU, albeit with potentially slower performance.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "The `AutoModel` class converts the token encodings to embeddings, and then feeds them through the encoder stack to return the hidden states.\n",
    "\n",
    "\n",
    "<center><b>Let's take a look at how we can extract these states from our corpus.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "<b>Bonus learning </b>\n",
    "</div>\n",
    "<font size =5 color = 'seagreen'> <b>Interoperability Between Frameworks<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Transformers is designed for seamless interoperability with TensorFlow and JAX.\n",
    "- Minimal code adjustments are necessary to load a pre-trained model in your preferred deep learning framework.\n",
    "- For instance, loading DistilBERT in TensorFlow can be achieved by utilizing the `TFAutoModel` class with just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- This interoperability proves exceptionally valuable when a model is exclusively available in one framework, but there's a desire to use it in another.\n",
    "- For instance,  the [XLM-RoBERTa model](https://huggingface.co/xlm-roberta-base) only comes with PyTorch weights. \n",
    "- However, if an attempt is made to load it in TensorFlow as below:\n",
    "\n",
    ">```python\n",
    ">tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    ">```\n",
    "<center> <b>an error will occur.</b></center>\n",
    "\n",
    "- To address such cases, you can include a `from_pt=True` argument in the `TfAutoModel.from_pretrained()` function, allowing the library to automatically download and convert the PyTorch weights for seamless use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- The ease of switching between frameworks in Transformers is evident.\n",
    "- In most instances, obtaining equivalent TensorFlow 2.0 classes is as simple as adding a \"TF\" prefix to the classes.\n",
    "- When utilizing the \"pt\" string (e.g., in the following section) as a shorthand for PyTorch, it can be effortlessly substituted with \"tf,\" denoting TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "<b>Bonus learning ends</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p4.2'> \n",
    "###### <font size = 6 color = 'pwdrblue'> **Extracting the last hidden states**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- As a warm-up, the task is to retrieve the last hidden states for a single string.\n",
    "- The process begins by encoding the string and converting the tokens into PyTorch tensors.\n",
    "- This conversion is facilitated by providing the return_tensors=\"pt\" argument to the tokenizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- As observed, the shape of the resulting tensor is `[batch_size, n_tokens]`.\n",
    "- With the encodings now in tensor form, the subsequent step involves placing them on the same device as the model.\n",
    "- The inputs are then passed in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- In this instance, the `torch.no_grad()` context manager has been employed to deactivate automatic gradient calculation.\n",
    "- This proves advantageous during inference, as it reduces the memory footprint of computations.\n",
    "- Depending on the model configuration, the output may consist of various objects, such as hidden states, losses, or attentions, organized in a class structure akin to a `namedtuple` in Python.\n",
    "- In our example, the model output takes the form of an instance of `BaseModelOutput`, with attributes accessible by name.\n",
    "- As the current model yields only one attribute, specifically the last hidden state, let's examine its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Upon examining the hidden state tensor, it becomes evident that its shape is `[batch_size, n_tokens, hidden_dim]`.\n",
    "- In simpler terms, a 768-dimensional vector is generated for each of the 6 input tokens.\n",
    "- In classification tasks, it is customary to use the hidden state corresponding to the `[CLS]` token as the input feature.\n",
    "- Given that this token consistently appears at the sequence's beginning, extraction can be accomplished by straightforwardly indexing into `outputs.last_hidden_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Having acquired the knowledge of obtaining the last hidden state for a single string, the next step involves replicating the process for the entire dataset. \n",
    "- This is achieved by establishing a new column, `hidden_state`, to store all these vectors.\n",
    "- Similar to the approach employed with the tokenizer, the `map()` method of `DatasetDict` will be utilized to extract all hidden states simultaneously.\n",
    "- The initial action involves encapsulating the preceding steps within a processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- The function differs from our previous logic only in the final step, where the ultimate hidden state is reverted to the CPU as a NumPy array.\n",
    "- It is crucial to note that when utilizing batched inputs with the`map()` method, the processing function must yield Python or NumPy objects.\n",
    "- Considering the model's expectation of tensors as inputs, the following step involves converting the `input_ids` and `attention_mask` columns to the \"torch\" format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **We can then go ahead and extract the hidden states across all splits in one go:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "\n",
    "**Note:**\n",
    "    \n",
    "- It's worth noting that in this instance, the `batch_size=None` was not specified; consequently, the default `batch_size=1000` is employed. \n",
    "- As anticipated, the application of the `extract_hidden_states(``) function has introduced a new hidden_state column to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Having acquired the hidden states corresponding to each tweet, the subsequent action involves training a classifier using these states.\n",
    "- For this purpose, a feature matrix is essential; let's examine the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p4.3'> \n",
    "###### <font size = 6 color = 'pwdrblue'> **Creating a feature matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- All the requisite information for training a classifier is now present in the preprocessed dataset.\n",
    "- The hidden states will serve as input features, and the labels will be utilized as targets.\n",
    "- The corresponding arrays in the familiar Scikit-Learn format can be effortlessly generated, as depicted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "\n",
    "**Note:**\n",
    "- Prior to training a model on the hidden states, it is advisable to conduct a sanity check to ascertain their effectiveness in representing the desired emotions for classification.\n",
    "- The subsequent section will demonstrate how visualizing the features offers a swift method for accomplishing this verification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p4.4'> \n",
    "###### <font size = 6 color = 'pwdrblue'> **Visualizing the training set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Given the complexity of visualizing hidden states in 768 dimensions, we opt for the powerful UMAP algorithm to project the vectors into 2D.\n",
    "- For optimal performance, UMAP$^*$ requires features scaled to the [0,1] interval, prompting the initial application of a `MinMaxScaler`.\n",
    "- Subsequently, the UMAP implementation from the `umap-learn` library is utilized to reduce the dimensionality of the hidden states.\n",
    "\n",
    "<div style='text-align: right;'><font size =2>  * Source : [L. McInnes, J. Healy, and J. Melville, [\"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\"](https://arxiv.org/abs/1802.03426), (2018).] \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- The outcome is an array with an equivalent number of training samples, now featuring only 2 features instead of the initial 768.\n",
    "- Further exploration of the compressed data is warranted, including the plotting of point density for each category individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "                   gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Note:**\n",
    "- Projections onto a lower-dimensional space are solely represented.\n",
    "- Overlapping in some categories does not imply their inseparability in the original space.\n",
    "- Conversely, if separable in the projected space, they will also be separable in the original space.\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Clear patterns are observable in this plot:\n",
    ">- Negative feelings like `sadness`, `anger`, and `fear` all inhabit analogous regions with slightly differing distributions.\n",
    ">- In contrast, `joy` and `love` are distinctly separated from negative emotions and share a similar space.\n",
    ">- `Surprise` is dispersed across various locations.\n",
    "- The lack of guaranteed separation is evident, as the model was not explicitly trained to discern differences between these emotions.\n",
    "- The model solely acquired implicit knowledge by predicting masked words in texts.\n",
    "</div>\n",
    "\n",
    "\n",
    "<font size = 5 color = 'seagreen'>\n",
    "    \n",
    "<center> <b>Now that we've gained some insight into the features of our dataset, let's finally train a model on it!</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p4.5'> \n",
    "###### <font size = 6 color = 'pwdrblue'> <b>Training a simple classifier</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "\n",
    "- Differences in hidden states among emotions have been observed, though an evident boundary is absent for several.\n",
    "- The hidden states will be employed for training a logistic regression model using Scikit-Learn.\n",
    "- The training of this straightforward model is rapid and does not necessitate GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# We increase `max_iter` to guarantee convergence \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- Upon evaluating accuracy, it may seem that our model is marginally superior to random chance; however, considering the unbalanced nature of the multiclass dataset, it proves to be significantly better.\n",
    "- To assess the model's performance, a comparison against a simple baseline is warranted.\n",
    "- Scikit-Learn provides a DummyClassifier capable of constructing a classifier using straightforward heuristics like always selecting the majority class or consistently opting for a random class.\n",
    "- In this scenario, the most effective heuristic involves consistently choosing the most frequent class, resulting in an accuracy of approximately 35%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "dummy_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- The baseline is outperformed significantly by our uncomplicated classifier employing DistilBERT embeddings.\n",
    "- A deeper understanding of the model's performance can be gained by examining the confusion matrix, revealing the association between true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- `Sadness` is the label with which `anger` and `fear` are most frequently confused, aligning with the observation during embeddings visualization.\n",
    "- `Joy` is commonly mistaken for both `love` and `surprise`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size =4>\n",
    "    \n",
    "**Note:**\n",
    "- The subsequent section will delve into the fine-tuning approach, known for yielding enhanced classification performance. \n",
    "- It is crucial to acknowledge that executing this necessitates additional computational resources, such as GPUs, which may not be accessible within your organization. \n",
    "- In such instances, a feature-based approach can serve as a viable compromise between traditional machine learning and deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <a id = 'p5'>\n",
    "    \n",
    "<font size = 10 color = 'midnightblue'> **Fine-Tuning Transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- The fine-tuning process for a transformer end-to-end will now be explored.\n",
    "- In this approach, the hidden states experience training rather than being utilized as fixed features, as demonstrated below.\n",
    "- A differentiable classification head is required, commonly attained by employing a neural network for classification.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img alt=\"encoder-tuning\" src=\"https://drive.google.com/uc?export=view&id=14itYn1bhOflvKCfErUk5bzMkTsWRPknT\" width = 800 height = 600>\n",
    "<center> <font size = 3><b>In the fine-tuning approach, the entire DistilBERT model undergoes training alongside the classification head.<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- Training the hidden states, which function as inputs to the classification model, helps mitigate the challenge of working with data potentially unsuitable for the classification task.\n",
    "- During training, the initial hidden states adapt, leading to a reduction in model loss and a subsequent enhancement in performance.\n",
    "\n",
    "- The Trainer API from Transformers will be utilized to streamline the training loop.\n",
    "- The necessary components to establish this setup will be examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p6'>\n",
    "<font size = 10 color = 'midnightblue'> **Loading a pretrained model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- The first requirement is a pre-trained `DistilBERT` model, akin to what was utilized in the feature-based approach.\n",
    "- A minor adjustment is made by opting for the `AutoModelForSequenceClassification` model instead of `AutoModel`.\n",
    "- The distinction lies in the fact that the `AutoModelForSequenceClassification` model features a classification head atop the pre-trained model outputs, facilitating straightforward training with the base model.\n",
    "- Specification of the number of labels the model needs to predict (six in our case) is essential, as this determines the number of outputs the classification head should have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size =4>\n",
    "    \n",
    "**Note:**\n",
    "    \n",
    "- A warning will be observed indicating that certain parts of the model are randomly initialized; this is standard procedure as the classification head is yet to undergo training.\n",
    "- The subsequent task involves specifying the metrics that will be employed to assess the model's performance during the fine-tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p6.1'>\n",
    "###### <font size = 6 color = 'pwdrblue'>**Defining the performance metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- To monitor metrics throughout training, a `compute_metrics()` function for the `Trainer` must be defined.\n",
    "- This function receives an `EvalPrediction` object, a named tuple with `predictions` and `label_ids` attributes, and is required to return a dictionary associating each metric's name with its value.\n",
    "- For our application, the computation of the model's $F_1$-score and accuracy will be executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "With the dataset and metrics prepared, two final tasks need attention before defining the Trainer class:\n",
    "\n",
    ">- Logging into our account on the Hugging Face Hub, facilitating the ability to push the fine-tuned model to our Hub account for community sharing.\n",
    ">- Defining all the hyperparameters for the training run.\n",
    ">- These steps will be addressed in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p6.2'>\n",
    "###### <font size = 6 color = 'pwdrblue'>**Training the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size =4>\n",
    "    \n",
    "If you're running this code in a Jupyter notebook, you can log in to the Hub with the following helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- A widget will be presented, allowing entry of the username and password or an access token with write privileges.\n",
    "\n",
    "- Detailed information on creating access tokens can be found in the Hub documentation.\n",
    "\n",
    "- For those working in the terminal, logging in can be accomplished by executing the following command:\n",
    "\n",
    "```bash\n",
    "$ huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- The `TrainingArguments` class is employed to articulate the training parameters.\n",
    "- This class holds a wealth of information, affording fine-grained control over the training and evaluation process.\n",
    "- The pivotal argument to specify is `output_dir`, serving as the storage location for all artifacts generated during training.\n",
    "- An exemplar showcasing the comprehensive structure of `TrainingArguments` is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True, \n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "\n",
    "- The batch size, learning rate, and number of epochs are configured here, and it is specified to load the best model upon completion of the training run.\n",
    "- Incorporating this final ingredient, our model can be instantiated and fine-tuned using the Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "    \n",
    "- In the logs, an $F_1$-score of approximately 92% on the validation set is observed for our model, representing a substantial improvement compared to the feature-based approach.\n",
    "- A more comprehensive analysis of the training metrics can be undertaken by computing the confusion matrix.\n",
    "- To visualize the confusion matrix, obtaining predictions on the validation set is the initial requisite.\n",
    "- The predict() method of the Trainer class furnishes several valuable objects suitable for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4> \n",
    "\n",
    "- The result of the `predict()` method is a `PredictionOutput` object encompassing arrays of `predictions` and `label_ids`, in addition to the metrics provided to the trainer.\n",
    "- Accessing the metrics on the validation set, for instance, can be achieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4> \n",
    "    \n",
    "- The `PredictionOutput` also includes the raw predictions for each class.\n",
    "- Decoding the predictions greedily is accomplished using `np.argmax()`.\n",
    "- This process produces the predicted labels, adopting the same format as the labels returned by the Scikit-Learn models in the feature-based approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size = 5 color = 'seagreen'> <b>With the predictions, we can plot the confusion matrix again:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- The confusion matrix is now much closer to the ideal diagonal setup.\n",
    "- The **'love'** category still exhibits frequent confusion with **'joy'** which is expected.\n",
    "- **'Surprise'** is often mistaken for **'joy'** and sometimes confused with **'fear'**.\n",
    "- While the model's overall performance appears commendable, it's essential to delve deeper into the specific error types before concluding our assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "    <b>Bonus learning </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Fine-Tuning with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- When utilizing TensorFlow, it is feasible to fine-tune models using the Keras API.\n",
    "- The primary distinction from the PyTorch API lies in the absence of a Trainer class, given that Keras models inherently incorporate a built-in fit() method.\n",
    "- To comprehend this process, let's commence by loading DistilBERT as a TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "tf_model = (TFAutoModelForSequenceClassification\n",
    "            .from_pretrained(model_ckpt, num_labels=num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Subsequently, our datasets will be transformed into the tf.data.Dataset format.\n",
    "- Leveraging the fact that our tokenized inputs are already padded, this transformation can be effortlessly achieved by employing the to_tf_dataset() method on emotions_encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column names to convert to TensorFlow tensors\n",
    "tokenizer_columns = tokenizer.model_input_names\n",
    "\n",
    "tf_train_dataset = emotions_encoded[\"train\"].to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "tf_eval_dataset = emotions_encoded[\"validation\"].to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=False,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Subsequently, our datasets will be transformed into the `tf.data.Dataset format`.\n",
    "- Leveraging the fact that our tokenized inputs are already padded, this transformation can be effortlessly achieved by employing the `to_tf_dataset()` method on `emotions_encoded`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "tf_model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 5>\n",
    "    <b>Bonus learning ends </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p7'>\n",
    "<font size = 10 color = 'midnightblue'>  **Error analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'>\n",
    "    \n",
    "<b><center> Before moving on, investigate the model's predictions a little bit further.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- A simple yet powerful technique is to sort the validation samples by the model loss. \n",
    "- When the label are passed during the forward pass, the loss is automatically calculated and returned. \n",
    "\n",
    "**Here's a function that returns the loss along with the predicted label:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
    "                             reduction=\"none\")\n",
    "\n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Using the `map()` method once more, we can apply this function to get the losses for all the samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# Convert our dataset back to PyTorch tensors\n",
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Finally, we create a `DataFrame` with the texts, losses, and predicted/true labels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "Now  `emotions_encoded` can be easily sorted by the losses in either ascending or descending order.\n",
    "\n",
    "**The goal of this exercise is to detect one of the following:**\n",
    "<font size = 4>\n",
    "    \n",
    "- _Wrong labels_: Every process that adds labels to data can be flawed. Annotators can make mistakes or disagree, while labels that are inferred from other features can be wrong. If it was easy to automatically annotate data, then we would not need a model to do it. Thus, it is normal that there are some wrongly labeled examples. With this approach, we can quickly find and correct them.\n",
    "\n",
    "- _Quirks of the dataset_: Datasets in the real world are always a bit messy. When working with text, special characters or strings in the inputs can have a big impact on the model's predictions. Inspecting the model's weakest predictions can help identify such features, and cleaning the data or injecting similar examples can make the model more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Let's first have a look at the data samples with the highest losses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "- The model's predictions include clear instances of incorrect labels.\n",
    "- Conversely, there are numerous examples without a distinct class, possibly indicating either mislabeling or the need for a new class.\n",
    "- Notably, the label `joy` appears to be misapplied multiple times.\n",
    "    \n",
    "<center><b>With this information the dataset can be refined, which often can lead to as big a performance gain (or more) as having more data or larger models!</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "\n",
    "- Examining samples with the lowest losses reveals that the model displays heightened confidence when predicting the sadness class. \n",
    "- Deep learning models excel at identifying and capitalizing on shortcuts to make predictions. \n",
    "- Consequently, it is valuable to invest time in scrutinizing examples where the model exhibits high confidence. \n",
    "- This ensures confidence that the model does not exploit specific text features improperly.\n",
    "    > <center> <b>So, let's also look at the predictions with the smallest loss:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- The revelation that joy is occasionally mislabeled and the model exhibits heightened confidence in predicting the label sadness is now established.\n",
    "- Armed with this insight, we can strategically enhance our dataset and monitor the class the model demonstrates high confidence in.\n",
    "- Prior to deploying the trained model, the final step involves saving it for future utilization.\n",
    "- Transformers provides a straightforward process for accomplishing this, as we'll elucidate in the upcoming section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p7'>\n",
    "<font size = 10 color = 'midnightblue'> **Saving and sharing the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<font size = 4>\n",
    "\n",
    "- The NLP community benefits greatly from sharing pretrained and fine-tuned models, and everybody can share their models with others via the Hugging Face Hub. \n",
    "- Any community-generated model can be downloaded from the Hub just like we downloaded the DistilBERT model. \n",
    "- With the `Trainer` API, saving and sharing a model is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "    \n",
    "- The fine-tuned model to make predictions on new tweets. \n",
    "- Since we've pushed our model to the Hub, we can now use it with the `pipeline()` function, just like we did in <<chapter_introduction>>. \n",
    "- First, let's load the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "from transformers import pipeline\n",
    "\n",
    "# Change `transformersbook` to your Hub username\n",
    "model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = 'seagreen'> **Then let's test the pipeline with a sample tweet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "<font size = 4>\n",
    "<b>\n",
    "Finally, we can plot the probability for each class in a bar plot. Clearly, the model estimates that the most likely class is `joy`, which appears to be reasonable given the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
