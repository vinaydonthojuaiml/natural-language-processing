{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COaHoagnathY"
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'>**Transformers - An introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUmJOPsoathb"
   },
   "source": [
    "## <center> <img alt=\"transformer-timeline\" caption=\"The transformers timeline\" src=\"https://drive.google.com/uc?export=view&id=1eucC4AUfAJW-rpprZ61i7U0FP9uXnAVF\" id=\"transformer-timeline\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV9st8otZPBT"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "\n",
    "- Understand basics of transformer models.\n",
    "- Explore transfer learning application in NLP\n",
    "- Learn applications of transformer model\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWbGeqoQZPBU",
    "tags": []
   },
   "source": [
    "# <a id= 'p0'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. History](#p1)<br>\n",
    "[2. Encoder decoder framework ](#p2)<br>\n",
    "[3. Attention mechanisms](#p3)<br>\n",
    "[4. Transfer learning in NLP](#p4)<br>\n",
    "[5. Transformer applications](#p5)<br>\n",
    ">[5.1 Text classification](#p5.1)<br>\n",
    ">[5.2 Named entity recognition](#p5.2)<br>\n",
    ">[5.3. Question answering](#p5.3)<br>\n",
    ">[5.4. Summarization](#p5.4)<br>\n",
    ">[5.5. Translation](#p5.5)<br>\n",
    ">[5.6. Text generation](#p5.6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iNlYYgeZPBU"
   },
   "source": [
    "## <a id = 'p1'>\n",
    "    \n",
    "<font size = 10 color = 'midnightblue'> **History**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSpfpiInathc"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## **Transformer Emergence (2017):**\n",
    "\n",
    "In 2017, Google researchers introduced the Transformer, a groundbreaking neural network architecture for sequence modeling, surpassing the performance of traditional recurrent neural networks (RNNs) in machine translation tasks.\n",
    "\n",
    "## **ULMFiT and Transfer Learning (Parallel Advancement):**\n",
    "\n",
    "<font size = 4> Simultaneously, the ULMFiT transfer learning method demonstrated that training LSTM networks on a vast and diverse corpus could yield top-tier text classifiers with minimal labeled data.\n",
    "\n",
    "## **Transformers' Evolution - GPT and BERT:**\n",
    "\n",
    "<font size = 4> The success of the Transformer architecture laid the foundation for two influential models: the Generative Pretrained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT).\n",
    "\n",
    "## **Unsupervised Learning Breakthrough:**\n",
    "\n",
    "<font size = 4> GPT and BERT, leveraging Transformer architecture and unsupervised learning, eliminated the need for task-specific architectures, setting new benchmarks in Natural Language Processing (NLP).\n",
    "\n",
    "## **Transformer Model Proliferation:**\n",
    "\n",
    "<font size = 4> Post-GPT and BERT, a multitude of transformer models have surfaced, reshaping the landscape of NLP benchmarks ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfNA71miathc"
   },
   "source": [
    "## <center> <img alt=\"transformer-timeline\" caption=\"The transformers timeline\" src=\"https://drive.google.com/uc?export=view&id=1D6oe9BD6DofpmfqgudxRAYNxGnjmePOZ\"  id=\"transformer-timeline\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvrg06SPZPBW"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdHRlWKAathe"
   },
   "source": [
    "## <a id = 'p2'>\n",
    "<font size = 10 color = 'midnightblue'> **The Encoder-Decoder Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hS19amq5athd"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## <font size = 6> To grasp the distinctive features of transformers, we must initially clarify:\n",
    "\n",
    "<font size = 4>\n",
    "    \n",
    "1. The encoder-decoder framework <br>\n",
    "2. Attention mechanisms <br>\n",
    "3. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUjUWOp3athd"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Before the advent of transformers, recurrent architectures, specifically LSTMs, stood as the pinnacle in NLP.<br>\n",
    "- LSTMs, characterized by a feedback loop in network connections, excelled in modeling sequential data, such as text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQkbv9Luathd",
    "tags": []
   },
   "source": [
    "## <font size = 5>Check the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnREB7c4athe"
   },
   "source": [
    "<center> <img alt=\"rnn\" caption=\"Unrolling an RNN in time.\" src=\"https://drive.google.com/uc?export=view&id=1B3Wbx0C2ztqOrbPzbojY8VswgZsz-xgC\" id=\"rnn\" width = 1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcJyeJBVZPBX"
   },
   "source": [
    "this is my worng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1knd0Nnathf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size=4>\n",
    "\n",
    "- In the left side illustration of figure above, an RNN processes input, which can be a word or character, sending it through the network.<br>\n",
    "- The RNN generates an output vector known as the hidden state as a result of processing the input.<br>\n",
    "- Simultaneously, the model establishes a feedback loop, feeding information back to itself for utilization in subsequent steps.<br>\n",
    "- When the loop is **\"unrolled\"** on the right side of figure, the RNN transmits information about its state at each step to the next operation in the sequence.<br>\n",
    "- This unrolling mechanism empowers the RNN to maintain a record of information from prior steps, enhancing its ability to make accurate output predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol2fwglzathf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "- Recurrent Neural Networks (RNNs) have been extensively applied in NLP, speech processing, and time series tasks.\n",
    "- RNNs played a crucial role in developing machine translation systems, particularly in tasks involving mapping sequences of words from one language to another.\n",
    "- For sequence-to-sequence tasks, like machine translation, the encoder-decoder architecture is commonly employed. The encoder encodes input information into a numerical representation (last hidden state), which is then utilized by the decoder to generate the output sequence.\n",
    "- Encoder and decoder components can adopt various neural network architectures capable of modeling sequences.\n",
    "- Figure below depicts a pair of RNNs encoding an English sentence (\"Transformers are great!\") into a hidden state vector, subsequently decoded to produce the German translation (\"Transformer sind grossartig!\"). The process involves sequential feeding of input and generation of output words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiVkNl0sathf"
   },
   "source": [
    "# <center> <img alt=\"enc-dec\" caption=\"Encoder-decoder architecture with a pair of RNNs. In general, there are many more recurrent layers than those shown.\" src=\"https://drive.google.com/uc?export=view&id=1gd2Y9r5Up3fPgk3o6pVJqCIyzRHqidOs\" id=\"enc-dec\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKWS7KkYathf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Despite its simplicity, a drawback of this architecture is the information bottleneck created by the final hidden state of the encoder. It must encapsulate the entire meaning of the input sequence, posing a challenge for long sequences where early information may be lost.\n",
    "- Handling long sequences becomes especially challenging as the architecture compresses all information into a single, fixed representation.\n",
    "- To address this bottleneck, a solution involves granting the decoder access to all of the encoder's hidden states.\n",
    "- The mechanism facilitating decoder access to all encoder hidden states is termed attention. This concept is integral to many contemporary neural network architectures.\n",
    "- Understanding the development of attention for RNNs serves as a foundational insight into one of the pivotal components of the Transformer architecture. Exploring this concept will enhance comprehension of modern neural network structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtF8y5j0ZPBX"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOTQz2fathf"
   },
   "source": [
    "# <a id = 'p3'>\n",
    "## <font size = 10 color = 'midnightblue'> **Attention mechanisms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jllrnkD8athf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Attention shifts from producing a single hidden state for the input sequence to generating a hidden state at each step, accessible by the decoder.\n",
    "- To avoid overwhelming the decoder, a mechanism is crucial for prioritizing which states to utilize among the multiple hidden states produced by the encoder.\n",
    "- Attention serves this purpose, enabling the decoder to assign varying weights, or \"attention,\" to each encoder state during every decoding timestep.\n",
    "- Figure below provides a visual representation of how attention operates, particularly in predicting the third token in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sJwwj3Uathg"
   },
   "source": [
    "# <center> <img alt=\"enc-dec-attn\" caption=\"Encoder-decoder architecture with an attention mechanism for a pair of RNNs.\" src=\"https://drive.google.com/uc?export=view&id=1wD64RcdtBR_gFAdk4ppOx5ulDjQX0D39\" id=\"enc-dec-attn\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6dZzVdFathg"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size =4>\n",
    "\n",
    "- Attention-based models concentrate on determining the most relevant input tokens at each timestep, facilitating the learning of intricate alignments between\n",
    "generated translations and source sentences.\n",
    "- Figure below offers a visualization of attention weights in an English-to-French translation model, with each pixel representing a weight.\n",
    "This visual representation showcases the model's ability to accurately align words, like \"zone\" and \"Area,\" even when their order differs in the two languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_EqvMxqathg"
   },
   "source": [
    "# <center><img alt=\"attention-alignment\" width=\"500\" caption=\"RNN encoder-decoder alignment of words in English and the generated translation in French (courtesy of Dzmitry Bahdanau).\" src=\"https://drive.google.com/uc?export=view&id=1kS7-UWxrGPu5HYgEYyRzqXbTOUAelWvi\" id=\"attention-alignment\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzEKYPemathg"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "<font size =4>\n",
    "\n",
    "- While attention improved translations, recurrent models for encoder and decoder suffered a major drawback—sequential computations,\n",
    "hindering parallelization across the input sequence.\n",
    "- The transformer introduced a revolutionary modeling paradigm, discarding recurrence entirely and relying on self-attention,\n",
    "a special attention form.\n",
    "- Self-attention is a concept where attention operates on all states within the same layer of the neural network.\n",
    "- Figure below illustrates this paradigm shift, showcasing both encoder and decoder employing self-attention mechanisms.\n",
    "The outputs feed into feed-forward neural networks (FF NNs), enabling faster training compared to recurrent models and contributing to recent breakthroughs in NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hitz1_Ujathg"
   },
   "source": [
    "<center> <img alt=\"transformer-self-attn\" caption=\"Encoder-decoder architecture of the original Transformer.\" src=\"https://drive.google.com/uc?export=view&id=1cBMFaVBjWKA--Tl8HykxPFnbql_XmEd9\" id=\"transformer-self-attn\" width = 900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpjT06Z1ZPBY"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmQnPQR3athg"
   },
   "source": [
    "# <a id = 'p4'>\n",
    "## <font size = 10 color = 'midnightblue'> **Transfer learning in NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plUNQURnathh"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "- <font size = 4>Common practice in computer vision involves employing transfer learning. A convolutional neural network like ResNet is initially trained on one task and\n",
    "then adapted or fine-tuned for a new task.\n",
    "\n",
    "- <font size = 4>The model is divided into a body and a head, where the body learns broad features of the source domain during training. The head is a task-specific network.\n",
    "\n",
    "- <font size = 4>During training, the body's weights acquire knowledge from the original task, initializing a new model for the subsequent task.\n",
    "This method outperforms traditional supervised learning, yielding high-quality models efficiently across various tasks with minimal labeled data.\n",
    "\n",
    "- <font size = 4>Figure below provides a visual comparison between traditional supervised learning and the transfer learning approach, highlighting the efficacy of the\n",
    "latter in producing versatile models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuY_sdz8athh"
   },
   "source": [
    "<center> <img alt=\"transfer-learning\" caption=\"Comparison of traditional supervised learning (left) and transfer learning (right).\" src=\"https://drive.google.com/uc?export=view&id=1gCyUlp9LppYg3F-xoIbTCaImPCN5cW7w\" id=\"transfer-learning\" width = 900>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z778GLAVZPBZ"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05dJj3A7athh"
   },
   "source": [
    "# <a id = 'p5'>\n",
    "## <font size = 10 color = 'midnightblue'> **Transformer Applications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJYGwdssathh"
   },
   "outputs": [],
   "source": [
    "text1 = '''Extremely disappointed with my recent iPhone purchase from Apple. The device constantly lags, and the battery life is abysmal,\n",
    "barely lasting through the day. Despite the hefty price tag, the performance is far from satisfactory. Customer support has been unhelpful,\n",
    "providing no viable solutions to address these persistant issues. This experience has left me regretting my decision to choose Apple,\n",
    "and I expected much better from a company known for its premium products.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X-BA-gWathi"
   },
   "outputs": [],
   "source": [
    "text2 = '''I recently purchased an iPhone from Apple, and it has been an absolute delight! The device runs smoothly, and the battery life is impressive, easily lasting throughout the day.\n",
    "The price, though high, is justified by the excellent performance and top-notch customer support. I am thoroughly satisfied with my decision to choose Apple, and it reaffirms their reputation\n",
    "for delivering premium products. Highly recommended for anyone seeking a reliable and high-performance smartphone'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJaOmoupathi"
   },
   "source": [
    "###### <a id = 'p5.1'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Text Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d540776efff340f99e0a0ab23cd6f1d5",
      "375ef419b88d482cbf237adcf2fe017c"
     ]
    },
    "id": "wB3cueqFathi",
    "outputId": "f1d517ff-ab72-4b28-cf89-fb4c82c77ecd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d540776efff340f99e0a0ab23cd6f1d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375ef419b88d482cbf237adcf2fe017c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rouCZf9lathj",
    "outputId": "ebeb18d3-b9cb-4c12-997d-503545fc6aa8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.999742"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "outputs1 = classifier(text1)\n",
    "pd.DataFrame(outputs1)\n",
    "\n",
    "# the output is label and corresponding probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNNEUrNgathj",
    "outputId": "48b4f48d-f5f8-426d-cd58-6dc26e25b5da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  POSITIVE  0.999818"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2 = classifier(text2)\n",
    "pd.DataFrame(outputs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLRaTTrLZPBb"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LShiYQlathj"
   },
   "source": [
    "###### <a id = 'p5.2'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Named entity recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c243400dd8fb42f58f3a5e14641b361d",
      "254f6f59b6044d04a7912c62dca05564",
      "c1c181b9bf1f4c949220756161c5369a",
      "d26d69a7a5ab4a41806f3aabb32963e0"
     ]
    },
    "id": "PniJbSGlathj",
    "outputId": "d7d78467-c1be-4d35-ddc9-5ed555b8a6d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c243400dd8fb42f58f3a5e14641b361d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254f6f59b6044d04a7912c62dca05564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c181b9bf1f4c949220756161c5369a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26d69a7a5ab4a41806f3aabb32963e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MISC</td>\n",
       "      <td>0.992854</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996596</td>\n",
       "      <td>Apple</td>\n",
       "      <td>59</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996131</td>\n",
       "      <td>Apple</td>\n",
       "      <td>396</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score    word  start  end\n",
       "0         MISC  0.992854  iPhone     38   44\n",
       "1          ORG  0.996596   Apple     59   64\n",
       "2          ORG  0.996131   Apple    396  401"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text1)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo3GjS7vZPBf"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N54MXrSwathj"
   },
   "source": [
    "###### <a id = 'p5.3'>\n",
    "###### <font size = 6 color = 'pwdrblue'> **Question answering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "e5194f6a22d242649a86abb966598140",
      "6964510d4f9f42df8bd520fbe1a70d08",
      "1b9332fc741b4190a626ab59cbf6b3a5",
      "e5d95f6016e04cdb998a9c1ec385453d",
      "8f8f0ed0cfbf42f19ba0016e8a81b996"
     ]
    },
    "id": "Brdw2Uv-athk",
    "outputId": "c68cc522-3767-46d2-a442-4f7e48c37a57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5194f6a22d242649a86abb966598140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964510d4f9f42df8bd520fbe1a70d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9332fc741b4190a626ab59cbf6b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d95f6016e04cdb998a9c1ec385453d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8f0ed0cfbf42f19ba0016e8a81b996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.843845</td>\n",
       "      <td>460</td>\n",
       "      <td>476</td>\n",
       "      <td>premium products</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score  start  end            answer\n",
       "0  0.843845    460  476  premium products"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\")\n",
    "question = \"What does the customer want?\"\n",
    "outputs = reader(question=question, context=text)\n",
    "pd.DataFrame([outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nE0tGlnlZPBf"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAOClEiwathk"
   },
   "source": [
    "###### <a id = 'p5.4'>\n",
    "###### <font size = 6 color = 'pwdrblue'>  **Summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stO5cZ_3athk",
    "outputId": "431f8870-b839-4333-e7de-41652197c981"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your min_length=56 must be inferior than your max_length=50.\n",
      "/usr/local/lib/python3.10/site-packages/transformers/generation/utils.py:1285: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (50). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extremely disappointed with my recent iPhone purchase from Apple. The device constantly lags, and battery life is abysmal,  barely lasting through the day. Customer support has been unhelpful, providing no viable\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "outputs = summarizer(text, max_length=50, clean_up_tokenization_spaces=True)\n",
    "print(outputs[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7agcwTrTZPBg"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrZK8pTLathk"
   },
   "source": [
    "###### <a id = 'p5.5'>\n",
    "###### <font size = 6 color = 'pwdrblue'>  **Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "1113d5efbaf5488194fe206e72bff6cd",
      "2b963b9b620442aeb89bf5dd62212a14",
      "aad137a6eda2431d8132dbdf544f9f3b",
      "ec8b305ec75041a6bbf449854b8a801d",
      "4ffc1d1199b146958b322b5bd0259e6f",
      "3b8de541acdf40ecabcb562c62cc2dbf",
      "9eb4001c45934bed812c206ca6d21b4d"
     ]
    },
    "id": "93q73l7fathk",
    "outputId": "2877ead7-7704-4894-bd27-efed61f4a157"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1113d5efbaf5488194fe206e72bff6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b963b9b620442aeb89bf5dd62212a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/298M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad137a6eda2431d8132dbdf544f9f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8b305ec75041a6bbf449854b8a801d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffc1d1199b146958b322b5bd0259e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8de541acdf40ecabcb562c62cc2dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb4001c45934bed812c206ca6d21b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Äußerst enttäuscht von meinem letzten iPhone-Kauf von Apple. Das Gerät ständig nachlässt, und die Batterie-Lebensdauer ist abgrundtief, kaum über den Tag. Trotz der kräftigen Preisschild, die Leistung ist bei weitem nicht zufriedenstellend. Kundensupport war nicht hilfreich, bietet keine tragfähigen Lösungen, um diese anhaltenden Probleme zu adressieren. Diese Erfahrung hat mich bedauert meine Entscheidung, Apple zu wählen, und ich erwartete viel besser von einem Unternehmen für seine Premium-Produkte bekannt.\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_de\",\n",
    "                      model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_Y9-Zi9ZPBg"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhoCHVWuatho"
   },
   "source": [
    "###### <a id = 'p5.6'>\n",
    "###### <font size = 6 color = 'pwdrblue'>  **Text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0TL9X0Sathp"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from transformers import set_seed\n",
    "set_seed(42) # Set the seed to get reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "53a4d67d59d5424db0a5981b869171b8",
      "baa631cd41134a8ea836e42626d5cfc5",
      "84967994697046fa878425c8e19cd878",
      "115aac48b5764fb1a779d1a038c10d77",
      "7c1675a81163401bab8dca832d26f139",
      "1ed7c538809a426c896dcd1b12dea98c"
     ]
    },
    "id": "j-lgStDIathp",
    "outputId": "00d609d9-4d84-479e-b35d-a395f4256669"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a4d67d59d5424db0a5981b869171b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa631cd41134a8ea836e42626d5cfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84967994697046fa878425c8e19cd878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115aac48b5764fb1a779d1a038c10d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1675a81163401bab8dca832d26f139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7c538809a426c896dcd1b12dea98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extremely disappointed with my recent iPhone purchase from Apple. The device constantly lags, and the battery life is abysmal, \n",
      "barely lasting through the day. Despite the hefty price tag, the performance is far from satisfactory. Customer support has been unhelpful, \n",
      "providing no viable solutions to address these persistent issues. This experience has left me regretting my decision to choose Apple, \n",
      "and I expected much better from a company known for its premium products.\n",
      "\n",
      "Customer service response:\n",
      "Dear Patron, Thanks for writing in! I am sorry to hear your experince with us. You have seen our products advertised, offered to and experienced with you, and have received very positive responses. I am so terribly sorry\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Patron, Thanks for writing in! I am sorry to hear your experience with us.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=150)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOZ6HYd2athp"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeiRF_tKZPBh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
