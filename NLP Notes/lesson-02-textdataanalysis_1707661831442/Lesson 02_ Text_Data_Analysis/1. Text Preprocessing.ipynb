{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b726cfeb-e2da-4d2f-b04f-7f48b0f411d4",
   "metadata": {
    "id": "b726cfeb-e2da-4d2f-b04f-7f48b0f411d4",
    "tags": []
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> <b>Text Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33217b9d-dd39-4643-bb3b-188c4fb4188c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "- Understand steps involved in text preprocessing\n",
    "- Implement text oreprocessing using  python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437063e-f8bd-41f1-89e9-53da3e9d3452",
   "metadata": {},
   "source": [
    "# <a id= 't0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of contents:**<br>\n",
    "[1. Installation and import of necessary packages](#t1)<br>\n",
    "[2. Download the necessary corpus from NLTK](#t2)<br>\n",
    "[3. Data cleaning steps](#t3)<br>\n",
    "> [3.1 Tokenization](#t3.1)<br>\n",
    "> [3.2 Changing case](#t3.2)<br>\n",
    "> [3.3 Spelling correction](#t3.3)<br>\n",
    "> [3.4 POS Tagging](#t3.4)<br>\n",
    "> [3.5 Named entity recognition (NER)](#t3.5)<br>\n",
    "> [3.6 Stemming and Lemmatization](#t3.6)<br>\n",
    ">> [a. Stemming](#3a)<br>\n",
    ">> [b. Lemmatization](#3b)<br>\n",
    "\n",
    "> [3.7 Noise entity removal](#t3.7)<br>\n",
    ">> [a. Remove stopwords](#a)<br>\n",
    ">> [b. Remove urls](#b)<br>\n",
    ">> [c. Remove punctuations](#c)<br>\n",
    ">> [d. Remove emoticons](#d)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b327445b-ee2e-4e1f-8cd0-4585b7988a9c",
   "metadata": {
    "id": "b327445b-ee2e-4e1f-8cd0-4585b7988a9c"
   },
   "source": [
    "##### <a id = 't1'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Installation and import of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdf780-7cdd-40b4-bd08-4512dd524453",
   "metadata": {
    "id": "55bdf780-7cdd-40b4-bd08-4512dd524453",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install re\n",
    "!pip install string\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f344be-7dcf-4b70-bac9-343b924b0924",
   "metadata": {
    "id": "58f344be-7dcf-4b70-bac9-343b924b0924",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dc234-cf9f-408c-a56c-da56408a6f3c",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ef5bd-796d-4cbd-acd2-1be72b2d08c6",
   "metadata": {
    "id": "de0ef5bd-796d-4cbd-acd2-1be72b2d08c6"
   },
   "source": [
    "##### <a id = 't2'>\n",
    "<font size = 10 color = 'midnightblue'> <b>Download necessary corpus and models from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6279c-fd1f-420e-955b-62563f75ab50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1431,
     "status": "ok",
     "timestamp": 1701922141715,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "0df6279c-fd1f-420e-955b-62563f75ab50",
    "outputId": "61da519c-23bd-4855-91f8-5be3e610fcbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pelwBrw-oZpf",
   "metadata": {
    "id": "pelwBrw-oZpf"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4> \n",
    "\n",
    "**Note:**\n",
    "    \n",
    "- A LoadError will be raised whenever there is a missing corpus or model which is a dependency for some other function.\n",
    "- Use `nltk.download( <name of the corpus/model> )` for downloading the requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b145e-0f14-4f57-9b29-960f2b6704ab",
   "metadata": {
    "id": "cd6b145e-0f14-4f57-9b29-960f2b6704ab"
   },
   "source": [
    "<font size = 6 color = seagreen> <b> Import the necessary corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724b372-f09c-429e-89e2-0c9a0c3e1a4f",
   "metadata": {
    "id": "6724b372-f09c-429e-89e2-0c9a0c3e1a4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22600d1-b227-481f-8b8f-447c1cc134a8",
   "metadata": {},
   "source": [
    "##### <a id = 't3'>\n",
    "<font size = 10 color = 'midnightblue'> <b> Data cleaning steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6bc37-fb57-461c-b917-f0ec85084fdd",
   "metadata": {
    "id": "86d6bc37-fb57-461c-b917-f0ec85084fdd"
   },
   "source": [
    "<font size = 6 color = seagreen> <b> <center> Let's start by defining a custom text for preprocessing.<br>\n",
    "<font size = 6 color = seagreen> <center>This text contains emoticons, punctuations urls etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e248600-0cee-435e-b553-01df6ec46ec7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "2e248600-0cee-435e-b553-01df6ec46ec7",
    "outputId": "bdbed5f4-0c36-4001-ea57-887d870f7be2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. ðŸš€\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0247bd17-d860-4fcb-b973-866b8f9c6664",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50756761-9f28-4c5d-ad9e-5cfa564dd6b7",
   "metadata": {
    "id": "b7e0ed52-ed1e-47d6-aeac-6c433ec43951",
    "tags": []
   },
   "source": [
    "<a id = 't3.1'>\n",
    "<font size = 6 color = pwdrblue>  <b>Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927e157-6de2-4d43-9c10-f22a2848291d",
   "metadata": {
    "id": "b7e0ed52-ed1e-47d6-aeac-6c433ec43951",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- This is a process of breaking the text into individual words or tokens.\n",
    "- This can be achieved by using word_tokenize or the simple split function associated with the string class.\n",
    "- In case of paragraph or larger documents, sentence tokenization can also be used.\n",
    "- Sentence tokenization is a crucial step in natural language processing (NLP) and text analysis, as it allows algorithms to work with smaller units of meaning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a3e24-ef79-414d-adc4-1ada1c211b5e",
   "metadata": {
    "id": "a50a3e24-ef79-414d-adc4-1ada1c211b5e"
   },
   "source": [
    "<font size = 5 color = seagreen>  <b>Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72cbc39-a51b-42f0-ac0f-35a959ec40ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "b72cbc39-a51b-42f0-ac0f-35a959ec40ca",
    "outputId": "95a1cacd-b02d-4348-9117-31dc86781fb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"{i}:  {sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ea73ff-3a0f-448e-8bf4-63aa04d942af",
   "metadata": {
    "id": "c7ea73ff-3a0f-448e-8bf4-63aa04d942af"
   },
   "source": [
    "<font size = 5 color = seagreen>  <b>Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d4649-0948-4b04-93b8-15e3f441bf2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922142190,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "e60d4649-0948-4b04-93b8-15e3f441bf2b",
    "outputId": "1c9b4bd1-fed9-4c55-fe05-630c2e5955bb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa24a6-b9ca-45af-8b1f-925bb60b2fd6",
   "metadata": {
    "id": "efaa24a6-b9ca-45af-8b1f-925bb60b2fd6"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**However, if the text contains emoticons or URLs, word tokenization may split them, complicating the text cleaning process. Hence, a simple text split function could be more helpful in this context.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baf231-8187-473a-90ee-b8e018efa15a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "d8baf231-8187-473a-90ee-b8e018efa15a",
    "outputId": "159d23b0-530c-4e19-a6f2-ffea21f8013a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_tokens = text.split()\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadabc46-045a-4e12-9cb5-62a582ab4566",
   "metadata": {
    "id": "cadabc46-045a-4e12-9cb5-62a582ab4566"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- <b>This also creates word tokens but keeps emoticons, urls, address handles, and hastags etc. together for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042e4a0-7dad-491f-8024-0fd27b0c7873",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60446923-8cab-43ec-8f30-ccce30596799",
   "metadata": {
    "id": "d0c48cb4-f273-4b2e-8a6c-3cf39774e289"
   },
   "source": [
    "<a id = 't3.2'>\n",
    "<font size = 6 color = pwdrblue>  <b>Changing the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c52234-01ac-4fb8-9f83-e7dc80baa732",
   "metadata": {
    "id": "d0c48cb4-f273-4b2e-8a6c-3cf39774e289"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Change of case is a text normalization process.\n",
    "- This process provides for uniform representation and reduces the vocabulary size.\n",
    "- Casing also eases the process of text matching, entity recognition, search and retrieval.\n",
    "- Changing the casing of the data reduces redundancy and helps the ML model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77a48b-aa48-4576-a615-9271439ae6bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "6f77a48b-aa48-4576-a615-9271439ae6bb",
    "outputId": "f392307d-c354-467c-d957-84a416d661b0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_lower_case = text.lower().split()\n",
    "print(words_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3870783-ebfd-43cc-a8b4-8ef5be5af7e1",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c1a004-e48d-46f1-ba54-063259c02c7c",
   "metadata": {
    "id": "adb51ddc-5eaa-4ec6-9e72-0365705a51fd"
   },
   "source": [
    "<a id = 't3.3'>\n",
    "<font size = 6 color = pwdrblue>  <b>Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a935e4b-418c-4b9b-b4e1-9ac0c106a096",
   "metadata": {
    "id": "27861661-efaa-49a4-98e0-a66bb29da4e8",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- This improves text quality and avoids miscommunication.\n",
    "- Spell correction helps support language models and embeddings.\n",
    "- Spell correction helps reducing ambiguity and handle out of vocabulary data.\n",
    "\n",
    "**For spelling correction we are using:**\n",
    " - `nltk.edit_distance` to measure distance between the words in the text and the vocabulary available in nltk.\n",
    " - `edit_distance` calculate the `Levenshtein edit-distance` between two strings to check similarity between words in the text and words of the valid vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff6a66-d569-47db-9980-c1e257e6ac2a",
   "metadata": {
    "id": "a3ff6a66-d569-47db-9980-c1e257e6ac2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e8f93-5f1f-4cbc-b40b-22aa6e8faffb",
   "metadata": {
    "id": "220e8f93-5f1f-4cbc-b40b-22aa6e8faffb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get list of English words\n",
    "words = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb872def-906c-4cf5-878d-c7bd806607df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701922142191,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "fb872def-906c-4cf5-878d-c7bd806607df",
    "outputId": "7c648468-96d7-4c26-c991-4d1ffac8bf55",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total number of words in the vocabulary : \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33d82e-a944-4cc4-bb31-2174d7522d42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 586186,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "9e33d82e-a944-4cc4-bb31-2174d7522d42",
    "outputId": "54553cd5-0aee-4513-d4cb-4d4fceb9bfcf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct spelling of each word\n",
    "corrected_tokens = []\n",
    "for token in word_tokens:\n",
    "    # Find the word with the lowest distance and replace it\n",
    "    corrected_token = min(words, key=lambda x: nltk.edit_distance(x, token))\n",
    "    corrected_tokens.append(corrected_token)\n",
    "print(\"Corrected tokens:\", corrected_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deed5ba-a6e2-4f2a-bff3-53c59db5238d",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc860cb-f3ca-4b46-8b4d-3cd3d4a19954",
   "metadata": {
    "id": "392de69a-8c76-4312-88d2-b222660319f2"
   },
   "source": [
    "<a id = 't3.4'>\n",
    "<font size = 6 color = pwdrblue>  <b>POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa8156-e0ae-4900-9bb3-58680e81954b",
   "metadata": {
    "id": "392de69a-8c76-4312-88d2-b222660319f2"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "Part-of-Speech tagging involves assigning words in a text corpus to specific parts of speech based on their definitions and contextual usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c2694-9413-4225-b05d-abbd66625ada",
   "metadata": {
    "id": "3d9c2694-9413-4225-b05d-abbd66625ada",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "word_tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6a528-4bf7-4c96-8ed7-35a6c69623a5",
   "metadata": {
    "id": "dba6a528-4bf7-4c96-8ed7-35a6c69623a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tagging can be done using pos_tag function of nltk.\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e106a-37ef-4815-87fd-f57ef786d285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "f56e106a-37ef-4815-87fd-f57ef786d285",
    "outputId": "d74c71c9-6c21-4de6-bd36-efc47a86df58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2ae1b3-044d-4312-a520-fe5ce81deb00",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c334f797-0000-40fa-a6a9-165ec3bf8631",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "<a id = 't3.5'>\n",
    "<font size = 6 color = pwdrblue>  <b>Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d0447-356f-483f-ab03-909cf41add00",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Named entity recognition (NER) is a natural language processing (NLP) technique that involves identifying and classifying entities (objects, places, people, organizations, dates, monetary values, percentages, etc.) in text.\n",
    "- Named entities can belong to various categories, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fae75-989a-4fbb-8a7f-758716b74ab9",
   "metadata": {
    "id": "cd8d504b-a253-4323-afe8-abe7c811e933"
   },
   "source": [
    "|**Entity Object**| **Meaning** |\n",
    "|-|-|\n",
    "|Person |Individual names of people.|\n",
    "|Location| Places, cities, countries, etc.|\n",
    "|Organization | Names of companies, institutions, etc.|\n",
    "|Date | Temporal expressions like dates and times.|\n",
    "|Money| Currency amounts.|\n",
    "|Percent| Percentage values.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O0RgoHSXrIxv",
   "metadata": {
    "id": "O0RgoHSXrIxv"
   },
   "source": [
    "<font size = 5 color = seagreen> <b><center> Let's consider a different example text to understand named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3784c-85c2-4ed6-8c1e-6f9fb42b9dc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "25a3784c-85c2-4ed6-8c1e-6f9fb42b9dc2",
    "outputId": "9e692cb1-11a6-463d-fdba-349c34e4bc2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_example = \"In 2019, Apple Inc. announced the launch of the iPhone 11 at their headquarters in Cupertino, California, with Tim Cook, the CEO, presenting the new features.\"\n",
    "print(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b12f928-89bd-42a8-8f1a-56aa0423a6cc",
   "metadata": {
    "id": "9b12f928-89bd-42a8-8f1a-56aa0423a6cc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "word_tokens = text_example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a436c8-492e-417a-b280-385096ce9a6d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "e5a436c8-492e-417a-b280-385096ce9a6d",
    "outputId": "c1810e58-c329-4e3e-8f23-a3fa605d4644",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the pos tags\n",
    "tagged = nltk.pos_tag(word_tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493befe4-99e9-4cbe-a00a-f5ca391f76ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922728374,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "493befe4-99e9-4cbe-a00a-f5ca391f76ae",
    "outputId": "93084bb4-b1cd-4194-e100-9cb52a7f13aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "named_entities = nltk.ne_chunk(tagged)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HEA3gWWCr75A",
   "metadata": {
    "id": "HEA3gWWCr75A"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Named entity recognition can also be implemented using spcay packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaca89d-1aa1-4b6c-9470-345ccd587595",
   "metadata": {
    "id": "daaca89d-1aa1-4b6c-9470-345ccd587595",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f540660-8342-4fa4-ab16-7c239a5900d1",
   "metadata": {
    "id": "5f540660-8342-4fa4-ab16-7c239a5900d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a nlp object of the text\n",
    "doc = nlp(text_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e92eef-b6b6-4b78-a130-fddccef678bd",
   "metadata": {
    "id": "70e92eef-b6b6-4b78-a130-fddccef678bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract named entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef92581-9e2f-42a5-a3fd-857019310cbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922729092,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "fef92581-9e2f-42a5-a3fd-857019310cbb",
    "outputId": "9307f5c2-ea62-401c-b329-344222a8c65d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the named entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e02fc1-ab0a-4dca-9026-029fdb6b616e",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d349d3f6-a243-4b2c-b1d0-6e67b9274b76",
   "metadata": {
    "id": "96c84d31-f526-484c-9f96-68b491423eaa"
   },
   "source": [
    "<a id = 't3.6'>\n",
    "<font size = 6 color = pwdrblue>  <b>Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3cf7f-8e70-4893-87be-8ce0d2202fc5",
   "metadata": {
    "id": "96c84d31-f526-484c-9f96-68b491423eaa"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Stemming and lemmatization are techniques used in NLP and text mining to reduce words to their base or root forms, simplifying the process of analysis and text understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b398fd-a9bc-4006-85b8-1f25fd979639",
   "metadata": {
    "id": "25b398fd-a9bc-4006-85b8-1f25fd979639",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. ðŸš€\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6hu8QVeBszFb",
   "metadata": {
    "id": "6hu8QVeBszFb"
   },
   "source": [
    "<font size = 5 color = seagreen> <b>Let's start by tokenising the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100190a-2996-4285-9691-7b901c14f4b1",
   "metadata": {
    "id": "1100190a-2996-4285-9691-7b901c14f4b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfa1258-a5ca-4378-abab-c10377c0526c",
   "metadata": {
    "id": "510323db-23a3-46b9-acc5-167fa8578f6c"
   },
   "source": [
    "<a id = '3a'>\n",
    "<font size = 5 color = seagreen> <b> Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec277cea-1a19-47ee-84f0-bbb186c6fc6c",
   "metadata": {
    "id": "510323db-23a3-46b9-acc5-167fa8578f6c"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Stemming is the process of removing suffixes or prefixes from words to obtain their root or base form, known as the stem. The goal is to reduce words to a common form, even if it is not a valid word.\n",
    "- Porter stemmer is one of the most used stemming technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c41f6-2360-411b-b5ee-6879b9f7c08e",
   "metadata": {
    "id": "b20c41f6-2360-411b-b5ee-6879b9f7c08e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create stemmer object\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11887788-9fbe-4ff7-956c-d078d95c7963",
   "metadata": {
    "id": "11887788-9fbe-4ff7-956c-d078d95c7963",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stem each token\n",
    "stemmed_tokens = [stemmer.stem(token) for token in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579770bc-0dec-4b50-b7d8-3198d108760f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922729092,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "579770bc-0dec-4b50-b7d8-3198d108760f",
    "outputId": "58f9af2b-6ce4-4868-a003-d4ae02127d2b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Stemmed tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350e9af-5f90-4aa2-8649-d61639718619",
   "metadata": {
    "id": "7c46765e-4f67-4f27-a0c3-e3188f982860"
   },
   "source": [
    "<a id = '3b'>\n",
    "<font size = 5 color = seagreen> <b> Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8b1ce-4017-4f09-91ec-2c5ea3250573",
   "metadata": {
    "id": "7c46765e-4f67-4f27-a0c3-e3188f982860"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma.\n",
    "- Lemmatization considers the context and meaning of a word and produces valid words.\n",
    "- NLTK provides wordnet based lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ff88c-84b2-4ba4-ad16-54fe56308e5d",
   "metadata": {
    "id": "478ff88c-84b2-4ba4-ad16-54fe56308e5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lemmatizer object\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab3221-a966-45b7-963b-d7d14a6a1d79",
   "metadata": {
    "id": "83ab3221-a966-45b7-963b-d7d14a6a1d79",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatize each token\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ca39e-6640-450d-85b7-94626bc87002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "556ca39e-6640-450d-85b7-94626bc87002",
    "outputId": "32ef84bf-9534-4b9b-f3de-006f8a558a09",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Lemmatized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4b602-9371-4e01-9b49-4216ae25cd13",
   "metadata": {
    "id": "66a4b602-9371-4e01-9b49-4216ae25cd13",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "**Using PoS tagging in lemmatization**\n",
    "  - For implementation of PoS tag based lemmatization, we pass the PoS tag for each word in the sentence.\n",
    "  - To acheive this we need to first map PoS tags from Penn Treebank to WordNet PoS tags.\n",
    "  - The below function performs the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce1f9-0613-4acf-b89a-e419b2aa9320",
   "metadata": {
    "id": "b28ce1f9-0613-4acf-b89a-e419b2aa9320",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pos tag mapping\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd785f-fda2-4b9e-a48d-4868be7ddd67",
   "metadata": {
    "id": "2afd785f-fda2-4b9e-a48d-4868be7ddd67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the pos tag\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42423b44-f903-4fbf-8b82-05f3f1a5ee42",
   "metadata": {
    "id": "42423b44-f903-4fbf-8b82-05f3f1a5ee42",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the root word for each of the tokens using their corresponding pos-tags\n",
    "lemma_sent = []\n",
    "for word, tag in tagged:\n",
    "    new_tag = pos_tagger(tag)\n",
    "    lemma = lemmatizer.lemmatize(word, new_tag)\n",
    "    lemma_sent.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fd56c-0a64-4fe8-8db6-825738f2f791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "431fd56c-0a64-4fe8-8db6-825738f2f791",
    "outputId": "1d92ac6c-91cd-44e0-daef-162b2b0decbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Original sentence : \\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e137d-7464-4b5a-9166-afdaa50c2919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701922731008,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "a63e137d-7464-4b5a-9166-afdaa50c2919",
    "outputId": "631830ef-c799-4a7c-96d0-8774d3c500ca",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Lemmatized sentence : \\n{' '.join(lemma_sent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec58bb2-43dc-4a9d-a502-1581bba7589d",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada9427-1388-48a6-a2a3-893739fd3e86",
   "metadata": {
    "id": "497cd5de-ba5b-45df-b79c-03479894db5c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id = 't3.7'>\n",
    "<font size = 6 color = pwdrblue>  <b>Noise entity removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d922159-43bf-4aea-a8eb-f559bf0a781d",
   "metadata": {
    "id": "497cd5de-ba5b-45df-b79c-03479894db5c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Noise entity removal in NLP involves the identification and removal of irrelevant or undesired entities from a given text.\n",
    "- Noise entities can be entities that are not relevant to the analysis or entities that add unnecessary complexity to the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1LqKYIBOvLpL",
   "metadata": {
    "id": "1LqKYIBOvLpL"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Embracing life's challenges is like navigating a journey. ðŸš€\n",
    "Stay motivated, overcome hurdles, and explore new paths to success!\n",
    "Check out inspiring stories at https://motivationalhub.com for an extra boost!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dc5a2d-31e8-49c0-adaa-bf4de5a2a3de",
   "metadata": {
    "id": "91dc5a2d-31e8-49c0-adaa-bf4de5a2a3de",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "word_tokens = text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ef019-97e2-4f5e-9f31-eb279cbdf38c",
   "metadata": {
    "id": "ce0ef019-97e2-4f5e-9f31-eb279cbdf38c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PoS tagging\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c94f62-3884-4177-8d3d-d8c0b6dad18d",
   "metadata": {
    "id": "c6c94f62-3884-4177-8d3d-d8c0b6dad18d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemma_sent = []\n",
    "for word, tag in tagged:\n",
    "    new_tag = pos_tagger(tag)\n",
    "    lemma = lemmatizer.lemmatize(word, new_tag)\n",
    "    lemma_sent.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace388e1-d33d-4ac1-9dd6-8c5b63b09232",
   "metadata": {},
   "source": [
    "<a id = 'a'>\n",
    "<font size = 5 color = seagreen> <b> a. Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600c2dc-13d4-45ab-82c3-ea7451e58278",
   "metadata": {
    "id": "c600c2dc-13d4-45ab-82c3-ea7451e58278"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "- Identify and remove common stopwords (e.g., \"is,\" \"the,\" \"and\") that do not carry much semantic meaning.\n",
    "- This can help in focusing on more meaningful entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387f0be-56b2-4ad9-aa23-97c693be5798",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "5387f0be-56b2-4ad9-aa23-97c693be5798",
    "outputId": "a9888af7-2076-40e3-a5d5-948a1df1e3dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain the list of stopwords from the corpus\n",
    "stp_wrds_eng = stopwords.words('english')\n",
    "print(stp_wrds_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d24e74-8391-47d0-8460-5a615dd9b3b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "49d24e74-8391-47d0-8460-5a615dd9b3b1",
    "outputId": "ea304338-ea6c-44d1-aa8f-4f7aabd2bc3c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "text_clean = [w for w in lemma_sent if w not in stp_wrds_eng]\n",
    "print(f\"Lemmatized : \\n{' '.join(lemma_sent)}\")\n",
    "print(f\"Cleaned  : \\n{' '.join(text_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c20b65-bbe0-4bc3-8fe6-4d71d7c4ec83",
   "metadata": {
    "id": "c7826e48-8e1f-43d4-9899-4dc59cab7dbf"
   },
   "source": [
    "<a id = 'b'>\n",
    "<font size = 5 color = seagreen> <b> b. Removing urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3e15e-7287-4ef3-bc53-62cb40e26746",
   "metadata": {
    "id": "c7826e48-8e1f-43d4-9899-4dc59cab7dbf"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Urls are not essential for many analysis process, hence they need to be removed.\n",
    "- We can use regex to identify and remove the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6ed43-2e2b-452f-adfc-507713d81ee4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "03f6ed43-2e2b-452f-adfc-507713d81ee4",
    "outputId": "e142698b-ebba-47ca-b0c5-31d0c9f26c4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identifying and substituting urls using the pattern 'https\\S+' for urls\n",
    "text_clean = re.sub(r'http\\S+', '', ' '.join(text_clean), flags=re.MULTILINE)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8656e60-beef-4c7a-b490-813722ef5a97",
   "metadata": {
    "id": "a0f7ef41-740c-4972-89f1-de238940fa2d"
   },
   "source": [
    "<a id = 'c'>\n",
    "<font size = 5 color = seagreen> <b> c. Remove punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e3da2-7b5d-485e-be9a-ac7b388eee42",
   "metadata": {
    "id": "a0f7ef41-740c-4972-89f1-de238940fa2d"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Punctuations are not always useful for anlaysis hence they shall also be removed.\n",
    "- The list of punctuations can be obtained from the `string` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fabf7a-c4d7-4db1-b9c0-0d7622400b6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "f1fabf7a-c4d7-4db1-b9c0-0d7622400b6c",
    "outputId": "aa18cdfd-d7b9-461b-c4b0-0990b46907cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_clean = [w for w in text_clean if w not in punctuation]\n",
    "print(f\"Cleaned  : \\n{''.join(text_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q3tqmRGTwY6m",
   "metadata": {
    "id": "Q3tqmRGTwY6m"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "\n",
    "**Note:**\n",
    "- In sentiment analysis sometimes punctuations like ! or ? may be significant for analysis.\n",
    "- Text cleaning steps should be customised based on the analysis objective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8974b-d0b7-4f6a-9508-ada717b371ab",
   "metadata": {
    "id": "c70685b6-e2d2-4260-8e95-24a51ef6c632"
   },
   "source": [
    "<a id = 'd'>\n",
    "<font size = 5 color = seagreen> <b>d. Remove emoticons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc47e3-b499-409f-ba53-79135f2dce7a",
   "metadata": {
    "id": "c70685b6-e2d2-4260-8e95-24a51ef6c632"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "- Most of the text from the social media is nowadays filled with emoticons.\n",
    "- Handling emoticons becomes a necessary part of NLP pipeline.\n",
    "- They may be directly removed for simplicity.\n",
    "- This can also be achieved using regex by specifying the unicodes for these emoticons as given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855f3067-4ee0-4752-93d1-92284a5eb072",
   "metadata": {
    "id": "855f3067-4ee0-4752-93d1-92284a5eb072",
    "tags": []
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "def strip_emoji(text):\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b25294-da9b-48d8-ab6a-74d91bd9708a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701922731731,
     "user": {
      "displayName": "Shubham Pandey",
      "userId": "10645658922718505789"
     },
     "user_tz": -330
    },
    "id": "c4b25294-da9b-48d8-ab6a-74d91bd9708a",
    "outputId": "23e136a5-5fc7-4239-f619-ba016796a0e0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use function to remove emoticons from text\n",
    "text_clean = strip_emoji(''.join(text_clean))\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DxssILQCzKdt",
   "metadata": {
    "id": "DxssILQCzKdt"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "\n",
    "**Note :**\n",
    " - Emoticons may be replaced with their intended meaning in form of text.\n",
    " - For example: ðŸ˜€ translates to  happy face.\n",
    " - This process is used in the vader sentiment package in data cleaning steps in sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebbf2dd-65ce-4660-8870-757d52ee7690",
   "metadata": {},
   "source": [
    "[top](#t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70865a9-c8a3-4eff-81a5-8614ce43baed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
