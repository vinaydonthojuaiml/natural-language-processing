{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78498308-74f7-4dba-9392-c8b2b0e7f44f",
   "metadata": {
    "id": "c1bdf106-ae3a-4ce5-b999-413d49b8f29e",
    "tags": []
   },
   "source": [
    "# <center> <font size = 24 color = 'steelblue'> <b> Twitter data sentiment analysis using NLTK - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f222ae-06f3-4884-9f6a-fabc4bfb2b0f",
   "metadata": {},
   "source": [
    "# <a id= 'p0'> \n",
    "<font size = 4>\n",
    "    \n",
    "**Table of contents:**<br>\n",
    "[1. Objective](#p1)<br>\n",
    "[2. Solution](#p2)<br>\n",
    ">[2.1. Import necessary packages](#p2.1)<br>\n",
    ">[2.2. Data acquisition](#p2.2)<br>\n",
    ">[2.3. Data cleaning](#p2.3)<br>\n",
    ">[2.4 Data exploration](#p2.4)<br>\n",
    ">[2.5 Model development](#p2.5)<br>\n",
    ">[2.6 Prediction for a single tweet input using the classifier](#p2.6)<br>\n",
    ">[2.7 Prediction for a single tweet input using the classifier](#p2.7)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc08c74-33f0-4a36-874b-1cced34ae2ff",
   "metadata": {},
   "source": [
    "##### <a id = 'p1'>\n",
    "<font size = 10 color = 'midnightblue'> <b> **Objective**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff886cb7-de9d-4ec8-b78d-3ac61ee1f99c",
   "metadata": {
    "id": "c1bdf106-ae3a-4ce5-b999-413d49b8f29e",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "\n",
    "- Objective of this project is to train a naive bayes classifier using the labeled twitter data and use the model for sentiment prediction.\n",
    "- For the given project, use the twitter sample data from nltk corpus for training this model.\n",
    "- The twitter sample corpus provides 3 files,\n",
    "    - positive_tweets: tweets labeled as positive sentiment tweets,\n",
    "    - negative_tweets: tweets labeled as negative sentiment tweets\n",
    "    - tweets.20150430-223406 : containing unlabeled tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e63b08-7fe7-46e7-8f5e-5d2d0cd8697b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### <a id = 'p2'>\n",
    "<font size = 10 color = 'midnightblue'> <b> Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6732d7d-352e-47b2-b9e3-6c73e0db3fef",
   "metadata": {
    "id": "c6732d7d-352e-47b2-b9e3-6c73e0db3fef",
    "tags": []
   },
   "source": [
    "<a id = 'p2.1'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue> **Import necessary packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de249cd-c8e5-4871-a222-8a65eee8dd07",
   "metadata": {
    "id": "4ba34314-004d-4628-bb04-8f13f483596c"
   },
   "outputs": [],
   "source": [
    "pip install advertools\n",
    "pip install vaderSentiment\n",
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec785241-0e46-499e-9d77-29e348ef861f",
   "metadata": {
    "id": "ec785241-0e46-499e-9d77-29e348ef861f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# for data cleaning :\n",
    "import re\n",
    "import advertools as adv # handling pictorial emojis\n",
    "from string import punctuation\n",
    "\n",
    "# Exploration and Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for model development\n",
    "import random\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Evaluation:\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93209542-b6d9-47ad-9a6e-0d621ea9c1cc",
   "metadata": {
    "id": "93209542-b6d9-47ad-9a6e-0d621ea9c1cc",
    "tags": []
   },
   "source": [
    "<font size = 5 color = seagreen><b> Downloading necessary nltk corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe15a1-e7d6-4b92-b14c-2b0b1d5c17aa",
   "metadata": {
    "id": "2bfe15a1-e7d6-4b92-b14c-2b0b1d5c17aa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download(\"twitter_samples\")\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad52e59-f7f2-4d81-b2b0-eec21300cace",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55428353-81d2-4eba-b986-bfab7014a0d1",
   "metadata": {
    "id": "55428353-81d2-4eba-b986-bfab7014a0d1",
    "tags": []
   },
   "source": [
    "<a id = 'p2.2'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b> Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b856f45-b0ca-4680-af9b-eece0467a917",
   "metadata": {
    "id": "9b856f45-b0ca-4680-af9b-eece0467a917",
    "outputId": "047c439c-e176-4eb0-988d-8b82c2e31ce0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.corpus.twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f2635-20a3-4d7c-9a83-c80a9a2a8056",
   "metadata": {
    "id": "d63f2635-20a3-4d7c-9a83-c80a9a2a8056",
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive = [tweet for tweet in nltk.corpus.twitter_samples.strings('positive_tweets.json')]\n",
    "negative = [tweet for tweet in nltk.corpus.twitter_samples.strings('negative_tweets.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6abce4b-1061-4f36-9546-c63d963cd3fa",
   "metadata": {
    "id": "b6abce4b-1061-4f36-9546-c63d963cd3fa"
   },
   "source": [
    "<font size = 5 color = seagreen><b> Create a collective labelled data set by labeling each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7968cfdd-ced5-4e35-85bc-a6a7a432a3db",
   "metadata": {
    "id": "7968cfdd-ced5-4e35-85bc-a6a7a432a3db",
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_tweets = [(p, 'pos') for p in positive] + [(n, 'neg') for n in negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8b313-d33e-49e4-a94e-679c83779b82",
   "metadata": {
    "id": "27a8b313-d33e-49e4-a94e-679c83779b82"
   },
   "source": [
    "<font size = 5 color = seagreen><b> Shuffle data to get random train and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd0816-44e4-4e1f-89a8-2fd7e5c44506",
   "metadata": {
    "id": "80bd0816-44e4-4e1f-89a8-2fd7e5c44506",
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186f872-3b11-40d2-8902-75ac1308fcbc",
   "metadata": {
    "id": "c186f872-3b11-40d2-8902-75ac1308fcbc",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<font size = 5 color = seagreen><b>  Consider first 1500 data rows as test and remaining as train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af1d03d-e8b0-4ca1-b60f-5528873a20e7",
   "metadata": {
    "id": "4af1d03d-e8b0-4ca1-b60f-5528873a20e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test  = labeled_tweets[:1500]\n",
    "train = labeled_tweets[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bef1df-b16b-4684-b84e-ecf5653c12a5",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5254b21-0896-4e23-9dc1-d96bf4c6bb79",
   "metadata": {
    "id": "59a9282d-2c9c-46eb-bbeb-bf1c0ff07a63",
    "tags": []
   },
   "source": [
    "<a id = 'p2.3'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b> Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0da9f-55cb-4a4f-8241-f2711af60d0f",
   "metadata": {
    "id": "59a9282d-2c9c-46eb-bbeb-bf1c0ff07a63"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "    \n",
    "**Implement basic data cleaning steps on this data such as:**\n",
    "  * Remove stopwords\n",
    "  * Remove punctuation\n",
    "  * Remove hyperlinks/urls\n",
    "  * Remove mentions i.e @abc_reader\n",
    "  * Remove any other additional symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729eb53-88c5-4fb1-b4a4-af98be7ffec9",
   "metadata": {
    "id": "59a9282d-2c9c-46eb-bbeb-bf1c0ff07a63"
   },
   "source": [
    "<font size = 5 color = seagreen><b> Define separate functions for each of the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cbcf6f-8a26-4195-b9fd-4e3e39f29067",
   "metadata": {
    "id": "30cbcf6f-8a26-4195-b9fd-4e3e39f29067",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for separating text and pictorial emojis\n",
    "def handling_emoji(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text =  emoji_pattern.sub(r'', tweet)\n",
    "    emoji_list = adv.extract_emoji(tweet)['emoji']\n",
    "    emojis = ' '.join([e[0] for e in emoji_list if len(e) >0 ])\n",
    "    return text + ' ' + emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1c7b0-b0f0-408d-b863-977175169b03",
   "metadata": {
    "id": "66f1c7b0-b0f0-408d-b863-977175169b03",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove punctuation using regex\n",
    "def removePunct(tweet):\n",
    "    pat = re.compile('[A-Za-z][{}]+'.format(punctuation))\n",
    "    txt = re.findall(pat, tweet)\n",
    "    if len(txt) > 0:\n",
    "        for t in txt:\n",
    "            tweet = tweet.replace(t[-1], '')\n",
    "        return tweet\n",
    "    else :\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b235f-d44f-4a79-b6c3-62c15fbe433e",
   "metadata": {
    "id": "b82b235f-d44f-4a79-b6c3-62c15fbe433e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove hyperlink/urls:\n",
    "def removeLinks(tweet):\n",
    "    pat = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(pat,'', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da60d82-0eaf-4290-8c46-88e3a8d93177",
   "metadata": {
    "id": "9da60d82-0eaf-4290-8c46-88e3a8d93177",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove mentions using regex:\n",
    "def removeMentions(tweet):\n",
    "    return re.sub(r'@[A-Za-z0-9_]+', '',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e7727-85d7-484f-ad08-3e231c673302",
   "metadata": {
    "id": "527e7727-85d7-484f-ad08-3e231c673302",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords from the text:\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def removeStpWrds(tokens):\n",
    "    return [words.lower() for words in tokens if words.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890464e-67cd-4c49-9ecc-c92968a69161",
   "metadata": {
    "id": "6890464e-67cd-4c49-9ecc-c92968a69161",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove symbols like # and words like \"RT\":\n",
    "def removeSymbols(tweet):\n",
    "    tweet = re.sub(r'#', '',tweet) # removed \"#\"\" symbol\n",
    "    tweet = re.sub(r'RT[\\s]+', '',tweet) # rmoved RT\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384500b6-c3e6-4393-b919-43b99289c812",
   "metadata": {
    "id": "384500b6-c3e6-4393-b919-43b99289c812"
   },
   "source": [
    "<font size = 5 color = seagreen><b> Create a common function for data cleaning implemneting all these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47675c1e-b10f-4ac0-9756-78d63cc84580",
   "metadata": {
    "id": "47675c1e-b10f-4ac0-9756-78d63cc84580",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_cleaning(tweet):\n",
    "    tweet  = handling_emoji(tweet)\n",
    "    tweet  = removeLinks(tweet)\n",
    "    tweet  = removeMentions(tweet)\n",
    "    tokens = removePunct(tweet).split()\n",
    "    tokens = removeStpWrds(tokens)\n",
    "    tokens = [w for w in tokens if w not in punctuation]\n",
    "    tweet  = ' '.join(tokens) # remove extra spaces\n",
    "    tweet  = removeSymbols(tweet) # remove \"#\" or RT\n",
    "    tweet = re.sub('[0-9]+','', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f1774-e26b-43ca-b1fc-648a4b11f741",
   "metadata": {
    "id": "761f1774-e26b-43ca-b1fc-648a4b11f741",
    "tags": []
   },
   "source": [
    "<font size = 5 color = seagreen><b> Apply the data-cleaning function on the train data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa004a5-ce3f-47cf-bf86-1684aaad408f",
   "metadata": {
    "id": "ffa004a5-ce3f-47cf-bf86-1684aaad408f",
    "outputId": "58e76c85-9fbb-464d-902d-c143c5ee5554",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_clean = []\n",
    "empty_twt_count =0\n",
    "for tweet, lab in train:\n",
    "    tweet = data_cleaning(tweet)\n",
    "    if len(tweet) >0 :\n",
    "        train_clean.append((tweet, lab))\n",
    "    elif len(tweet) == 0:\n",
    "        empty_twt_count_neg += 1\n",
    "print(train_clean[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31372036-a543-4071-8c6c-ab8c92205768",
   "metadata": {
    "id": "31372036-a543-4071-8c6c-ab8c92205768"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4> \n",
    "    \n",
    "**Note:**\n",
    "- Additionally a spell check can be performed to remove any irrelevant words.\n",
    "- However, when assuming that the irrelevant mispelled words are not repeated a lot and hence will not affect the analysis significantly.\n",
    "- Data cleaning step for text data analysis may incorporate a lot of other steps based on the text data quality and objective of the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f2839-44bd-4e2a-aa66-7fbd796b62e6",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2581796-7a87-4661-948d-b19fe1666c10",
   "metadata": {
    "id": "b1ecdba6-e0fe-47f7-9516-349a5416807c",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id = 'p2.4'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b> Explore the data<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d33fd6c-8347-4360-9f29-b1a188fb7680",
   "metadata": {
    "id": "b1ecdba6-e0fe-47f7-9516-349a5416807c"
   },
   "source": [
    "<font size =5 color = seagreen><b>  Study distribution of pos to neg tweets in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aaca40-62e1-46af-be09-46f872371098",
   "metadata": {
    "id": "57aaca40-62e1-46af-be09-46f872371098",
    "outputId": "0b47a143-8d3f-472c-8799-5714863f80f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.Series([label for tweet, label in train]).value_counts().plot.pie(cmap = 'Set3', autopct = \"%.2f %%\")\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e26dc-2135-465d-8a71-ac3b3948eaaa",
   "metadata": {
    "id": "909e26dc-2135-465d-8a71-ac3b3948eaaa"
   },
   "source": [
    "<font size =5 color = seagreen><b>  Extract positive and negative tweets from train data for frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aa880-5b3b-4b47-b620-634b76164faf",
   "metadata": {
    "id": "eb5aa880-5b3b-4b47-b620-634b76164faf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_clean = [p for p, l in train_clean if l == 'pos' ]\n",
    "neg_clean = [n for n, l in train_clean if l == 'neg' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c0d6a-6084-4671-9de4-00f1ec2c516e",
   "metadata": {
    "id": "6d3c0d6a-6084-4671-9de4-00f1ec2c516e",
    "tags": []
   },
   "source": [
    "<font size =5 color = seagreen><b>  Visually analyze the positive and negative tweets using wordcloud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4cd21-6ee5-420b-ad1b-97348f122deb",
   "metadata": {
    "id": "52e4cd21-6ee5-420b-ad1b-97348f122deb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_words_freq = nltk.FreqDist(\" \".join(pos_clean).split())\n",
    "wordcloud_pos = WordCloud(width = 800, height = 600,\n",
    "                      background_color = \"white\", colormap = 'viridis',\n",
    "                     max_words = 50)\n",
    "wc_pos = wordcloud_pos.generate_from_frequencies(frequencies=positive_words_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80d4c1-9217-44ce-af6b-1665969d3532",
   "metadata": {
    "id": "cf80d4c1-9217-44ce-af6b-1665969d3532",
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_words_freq = nltk.FreqDist(\" \".join(neg_clean).split())\n",
    "wordcloud_neg = WordCloud(width = 800, height = 600,\n",
    "                      background_color = \"white\", colormap = 'Accent',\n",
    "                     max_words = 50)\n",
    "wc_neg = wordcloud_neg.generate_from_frequencies(frequencies=negative_words_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71687c16-8d74-4d7c-a4fb-d7926b3c1608",
   "metadata": {
    "id": "71687c16-8d74-4d7c-a4fb-d7926b3c1608",
    "outputId": "c1478dbe-0b92-4865-99e6-5b2b58a78296"
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2, figsize = (20,8))\n",
    "ax[0].set_title(\"Positive Tweets\", size = 30, pad = 22, weight = 'bold')\n",
    "ax[0].imshow(wordcloud_pos, interpolation=\"bilinear\")\n",
    "ax[1].set_title(\"Negative Tweets\", size = 30, pad = 22, weight = 'bold')\n",
    "ax[1].imshow(wordcloud_neg, interpolation=\"bilinear\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04712f10-6f0b-4df4-affb-9d04ac062b49",
   "metadata": {
    "id": "4cf14a8f-85ee-4f19-8fe4-4a44c7d55a1b"
   },
   "source": [
    "<font size =5 color = seagreen><b> Create a vocabulary using these tweets to generate features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c0890-53eb-48c3-973e-c0c29f99aaad",
   "metadata": {
    "id": "4cf14a8f-85ee-4f19-8fe4-4a44c7d55a1b"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**Use only words in train data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdf06d-172b-4442-9208-c6b0b3f9c1e7",
   "metadata": {
    "id": "57bdf06d-172b-4442-9208-c6b0b3f9c1e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "strings = [w for w, l in train_clean]\n",
    "words = ' '.join(strings).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240ec2d-f8fd-4bfa-94cf-a545b33fd463",
   "metadata": {
    "id": "a240ec2d-f8fd-4bfa-94cf-a545b33fd463",
    "outputId": "add39df2-2949-473a-f894-958ce3866cd6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total word length = {len(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82b84a-b3bb-431a-87d4-147d54abdfd3",
   "metadata": {
    "id": "6b82b84a-b3bb-431a-87d4-147d54abdfd3",
    "outputId": "e96b27a2-533f-4eb9-9683-829df30ec4bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_words = set(words)\n",
    "print(f\"Total no. of unique words = {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332d271-1dd9-4d2e-be80-21d6e2a9c160",
   "metadata": {
    "id": "6332d271-1dd9-4d2e-be80-21d6e2a9c160"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**Use this vocabulary to generate features manually:**\n",
    "- Taking the simplest case, we will generate features based on presence or absence of a word in the labeled tweet.\n",
    "- It will generate a fundamental sparse matrix with labels.\n",
    "- For campatibility with NLTK naive Bayes, we need a feature dictionary for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703f7b5-0fa6-4dda-b55a-eb94e53b6105",
   "metadata": {
    "id": "6703f7b5-0fa6-4dda-b55a-eb94e53b6105",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction(tweet):\n",
    "    return {f'contains_{w}' : int(w in tweet) for w in unique_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab189cd-c0c2-46dd-8e42-7b4a9e355957",
   "metadata": {
    "id": "7ab189cd-c0c2-46dd-8e42-7b4a9e355957",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features \u001b[38;5;241m=\u001b[39m [(feature_extraction(t), l) \u001b[38;5;28;01mfor\u001b[39;00m t,l \u001b[38;5;129;01min\u001b[39;00m train_clean]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_clean' is not defined"
     ]
    }
   ],
   "source": [
    "train_features = [(feature_extraction(t), l) for t,l in train_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c000e00-dcb6-49ae-9612-9b7189073b56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id = 'p2.5'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b> Model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d9dda-a4fa-46cc-8713-5681c43834e3",
   "metadata": {
    "id": "528d9dda-a4fa-46cc-8713-5681c43834e3",
    "tags": []
   },
   "source": [
    "<font size =5 color = seagreen><b>  Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44eba1e-0063-4334-9898-4144a68952bd",
   "metadata": {
    "id": "d44eba1e-0063-4334-9898-4144a68952bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eVXEeXaprKZY",
   "metadata": {
    "id": "eVXEeXaprKZY"
   },
   "source": [
    "<font size =5 color = seagreen><b> Check accuracy on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f01ec5-b53f-408c-9330-ecc3e486e5d6",
   "metadata": {
    "id": "59f01ec5-b53f-408c-9330-ecc3e486e5d6",
    "outputId": "29835b08-f0fb-4182-bbab-2125c100d4d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc_train = nltk.classify.accuracy(clf, train_features)\n",
    "print(\"Accuracy on train data : \", acc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bNUtySjdrjfj",
   "metadata": {
    "id": "bNUtySjdrjfj"
   },
   "source": [
    "<font size =5 color = seagreen><b>  Get the most informative features to understand the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb1c07-430d-4c0e-877c-3539dcc9cdb1",
   "metadata": {
    "id": "1efb1c07-430d-4c0e-877c-3539dcc9cdb1",
    "outputId": "187aba43-2a6b-4bc5-880b-18c2aa02db38",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.show_most_informative_features(n = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d85ee-0f18-43d1-a283-2008d5f2e5cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25850b3c-396f-4457-aae6-9776ad2e2e3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<a id = 'p2.6'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b> Prediction for a single tweet input using the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90abfb9-1ee1-439d-bced-d5858e89e13c",
   "metadata": {
    "id": "e90abfb9-1ee1-439d-bced-d5858e89e13c"
   },
   "source": [
    "<font size =5 color = seagreen><b>   Use this model to classify a new tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838248ce-0833-4383-9bbd-851812900fff",
   "metadata": {
    "id": "838248ce-0833-4383-9bbd-851812900fff",
    "outputId": "6ecbf31f-0406-43e1-e12a-626132eefffb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_tweet = \"Absolutely loving the new features in the latest update! The user interface is sleek, and the performance is top-notch. Great job, @ProductTeam! 👏 #HappyCustomer #ProductLove\"\n",
    "print(sample_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be70387-3b4e-4f4b-b9bf-ef17d447370b",
   "metadata": {
    "id": "6be70387-3b4e-4f4b-b9bf-ef17d447370b",
    "tags": []
   },
   "source": [
    "<font size =5 color = seagreen><b>   Clean the tweet and extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0362954-06e5-4959-98cc-fe7470dac600",
   "metadata": {
    "id": "f0362954-06e5-4959-98cc-fe7470dac600",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_tweet = data_cleaning(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7891b10-5518-430d-b5bd-4a2fa2405649",
   "metadata": {
    "id": "c7891b10-5518-430d-b5bd-4a2fa2405649",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_set = feature_extraction(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c82e3ce-1dce-40a2-a49d-dad7128db718",
   "metadata": {
    "id": "5c82e3ce-1dce-40a2-a49d-dad7128db718",
    "outputId": "3dd9aab1-b669-4231-b5db-466031d64086",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.classify(feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e92a48-1728-440d-b8fd-ea20b889308e",
   "metadata": {
    "id": "88e92a48-1728-440d-b8fd-ea20b889308e"
   },
   "source": [
    "<font size =5 color = seagreen><b>   Get sentimet score using the textblob and vadersentiment packages without employing any data cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748bc8d-f748-49f8-846e-f75ff254725a",
   "metadata": {
    "id": "1748bc8d-f748-49f8-846e-f75ff254725a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getAnalysis(score):\n",
    "    if score <= 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lYxMjBifr9-x",
   "metadata": {
    "id": "lYxMjBifr9-x"
   },
   "source": [
    "<font size =5 color = seagreen><b>   Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ba935-9a65-48ff-a035-48e6f0f6c11e",
   "metadata": {
    "id": "c81ba935-9a65-48ff-a035-48e6f0f6c11e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a text blob object of the text\n",
    "blob = TextBlob(sample_tweet)\n",
    "\n",
    "# get the sentiment object with polarity value\n",
    "sentiment_obj = blob.analyzer.analyze(sample_tweet)\n",
    "\n",
    "# get the sentiment based on polarity value\n",
    "sentiment = getAnalysis(blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c587df9-e54b-4af0-973b-bba2cf6b2650",
   "metadata": {
    "id": "8c587df9-e54b-4af0-973b-bba2cf6b2650",
    "outputId": "4d31271b-ff5a-4c98-eaec-e1096d52da46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"Text: \", sample_tweet)\n",
    "print(\"Sentiment object : \", sentiment_obj)\n",
    "print(\"Sentiment : \",sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tsrvb_wAr7SD",
   "metadata": {
    "id": "tsrvb_wAr7SD"
   },
   "source": [
    "<font size =5 color = seagreen><b>   Using vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f59569-da4d-47ce-b1ce-6acf92f07ec7",
   "metadata": {
    "id": "43f59569-da4d-47ce-b1ce-6acf92f07ec7",
    "outputId": "17013adb-160a-4f4b-f0a5-d2e68e80ca82",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# obtain the polarity scores\n",
    "vs = analyzer.polarity_scores(sample_tweet)\n",
    "\n",
    "# display results\n",
    "print(\"{} \\n{}\".format(sample_tweet, str(vs)))\n",
    "print(\"Sentiment : \",getAnalysis(vs['compound']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a48d6-49be-4b83-9294-eabc8d5e1296",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X9rwJDaDsGB7",
   "metadata": {
    "id": "X9rwJDaDsGB7",
    "tags": []
   },
   "source": [
    "<a id = 'p2.7'>\n",
    "    \n",
    "## <font size = 6 color = pwdrblue><b>  Prediction for the test-set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553e20a-a700-4e6d-88e7-7f935047cb3f",
   "metadata": {
    "id": "4553e20a-a700-4e6d-88e7-7f935047cb3f"
   },
   "source": [
    "<font size =5 color = seagreen><b> Implement cleaning and feature extraction steps for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e94c2b-ead6-4feb-8cb9-fb9c2222f6ac",
   "metadata": {
    "id": "e1e94c2b-ead6-4feb-8cb9-fb9c2222f6ac",
    "outputId": "175c1ffa-12ce-452f-c6b0-cd0f06fe4d32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_set = []\n",
    "test_feature_set = []\n",
    "test_labels = []\n",
    "\n",
    "for tweet, lab in test:\n",
    "    tweet_clean = data_cleaning(tweet)\n",
    "    if len(tweet) >0 :\n",
    "        features = feature_extraction(tweet_clean)\n",
    "        test_set.append((features, lab))\n",
    "        test_feature_set.append(features)\n",
    "        test_labels.append(lab)\n",
    "    elif len(tweet) == 0:\n",
    "        empty_tweet += 1\n",
    "print('Count of cleaned test tweets :', len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROo4gXXJsPB_",
   "metadata": {
    "id": "ROo4gXXJsPB_"
   },
   "source": [
    "<font size =5 color = seagreen><b>Classify using trained classifer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d9a55-dae5-4019-8a3c-b0bc3e584c20",
   "metadata": {
    "id": "1c5d9a55-dae5-4019-8a3c-b0bc3e584c20",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_nb = clf.classify_many(test_feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BJDp7VNUsULK",
   "metadata": {
    "id": "BJDp7VNUsULK"
   },
   "source": [
    "<font size =5 color = seagreen><b> Evaluate the model by getting classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47ab29-f3fb-4a1e-a14e-5da46cc352a3",
   "metadata": {
    "id": "cf47ab29-f3fb-4a1e-a14e-5da46cc352a3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_mat_nb = pd.crosstab(index = test_labels, columns = test_pred_nb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418937c9-cc19-48a9-b7bf-8932a51381a5",
   "metadata": {
    "id": "418937c9-cc19-48a9-b7bf-8932a51381a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cs_nb  =classification_report(y_true = test_labels, y_pred = test_pred_nb )\n",
    "print(cs_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91212d-f3b8-4c59-b0f3-9eefb96f1cd8",
   "metadata": {
    "id": "2c91212d-f3b8-4c59-b0f3-9eefb96f1cd8",
    "tags": []
   },
   "source": [
    "<font size =5 color = seagreen><b> Get prediction and accuracies for test set using TextBlob and vade sentiment models also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149618e-03d3-417f-a4cd-4e78d74c15b4",
   "metadata": {
    "id": "0149618e-03d3-417f-a4cd-4e78d74c15b4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction using TextBlob\n",
    "def classify_tb(tweet):\n",
    "    # create a text blob object of the text\n",
    "    blob = TextBlob(tweet)\n",
    "\n",
    "    # get the sentiment object with polarity value\n",
    "    sentiment_obj = blob.analyzer.analyze(tweet)\n",
    "\n",
    "    # get the sentiment based on polarity value\n",
    "    sentiment = getAnalysis(blob.sentiment.polarity)\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea545b61-bb57-491b-ae70-d0c0c87caf07",
   "metadata": {
    "id": "ea545b61-bb57-491b-ae70-d0c0c87caf07",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_tb = [classify_tb(tweet) for tweet, lab in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70da32-4103-4046-a70c-b3cb384595b0",
   "metadata": {
    "id": "dd70da32-4103-4046-a70c-b3cb384595b0"
   },
   "outputs": [],
   "source": [
    "test_pred_tb = list(map(lambda x: x.lower()[:3], test_pred_tb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4bab1-6baf-4488-812b-4b191b7b9148",
   "metadata": {
    "id": "bac4bab1-6baf-4488-812b-4b191b7b9148",
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_mat_tb = pd.crosstab(index = test_labels, columns = test_pred_tb )\n",
    "cf_tb = classification_report(y_true = test_labels, y_pred = test_pred_tb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9275f5-40b1-47ec-a131-1a1d572586e3",
   "metadata": {
    "id": "1c9275f5-40b1-47ec-a131-1a1d572586e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prediction using Vader\n",
    "def usingVader(tweet):\n",
    "    # obtain the polarity scores\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    sentiment = getAnalysis(vs['compound'])\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d5095-9a10-4ceb-bc9c-a4c8c7534632",
   "metadata": {
    "id": "320d5095-9a10-4ceb-bc9c-a4c8c7534632",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_vs = [usingVader(tweet) for tweet, lab in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61aeffb-0ac2-4530-a975-33e38f10860b",
   "metadata": {
    "id": "b61aeffb-0ac2-4530-a975-33e38f10860b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_vs = list(map(lambda x: x.lower()[:3], test_pred_vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94214eab-8d22-4f42-9f91-14c4d2b776a4",
   "metadata": {
    "id": "94214eab-8d22-4f42-9f91-14c4d2b776a4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_mat_vs = pd.crosstab(index = test_labels, columns = test_pred_vs )\n",
    "cf_vs = classification_report(y_true = test_labels, y_pred = test_pred_vs )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md3qOC57s19j",
   "metadata": {
    "id": "md3qOC57s19j"
   },
   "source": [
    "<font size =5 color = seagreen><b> Disaplay all results and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7ccbb-4e01-42b3-8c10-1f085b852b12",
   "metadata": {
    "id": "ddb7ccbb-4e01-42b3-8c10-1f085b852b12",
    "outputId": "1b4ba42e-f34d-44ef-f180-83d45ea1dee0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nNaive Bayes :\\n\\n\", cs_nb)\n",
    "print(\"\\nText Blob :\\n\\n\", cf_tb)\n",
    "print(\"\\nVader :\\n\\n\",cf_vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0418e8-32cc-4889-90b6-01da0ad81fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hKQp1fXs7a0",
   "metadata": {
    "id": "8hKQp1fXs7a0"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4> \n",
    "\n",
    "**Conclusion:**\n",
    "* The manually trained classifier (Naive Bayes) gets a very accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333383b-ee20-465d-b778-8c364eda7076",
   "metadata": {
    "id": "4bcbcba0-9a22-440b-a849-c089591a3c4e"
   },
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8a2c3-2e89-4d79-979d-a0990d004659",
   "metadata": {
    "id": "31e8a2c3-2e89-4d79-979d-a0990d004659"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98db6f-a66b-4611-96d5-cc1448f12716",
   "metadata": {
    "id": "7a98db6f-a66b-4611-96d5-cc1448f12716"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
